{"./":{"url":"./","title":"前言","keywords":"","body":"Ceph Handbook 此书是ceph的阅读与理解，帮助大家更快了解ceph的管理和使用。 GitHub地址：https://github.com/eiuapp/ceph-handbook 在线访问地址：https://eiu.app/ceph-handbook ceph是著名的存储系统。 下图是ceph生态圈图： 相关资源 关于 本书中引用了一些公开的分享与链接并加以整理。 本书作于2019年初，会持续更新。 Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/env/env.html":{"url":"docs/env/env.html","title":"环境","keywords":"","body":"这里是贯穿本书的环境 env-1 软件 os: windows 10 命令行： git-bash或者cmder或者cmd virtual: 6.0.4 vagrant: 2.2.3 vagrant box: ubuntu/bionic64 ceph：V12.2.11 LUMINOUS ceph-deploy：1.5.38 硬件 host: virtualbox 虚拟机 cpu: 2 RAM: 1GB 架构部署 IP 部件 OSD Monitor Managers MDSs 172.21.12.10 ceph-admin 172.21.12.11 ceph-client 172.21.12.12 ceph-server-1 Y Y 172.21.12.13 ceph-server-2 Y 172.21.12.14 ceph-server-3 Y env-2 软件 os：Ubuntu Server 16.04×64 Python: 2.7.12 ceph：v13.2.4 Mimic ceph-deploy：2.0.1 硬件 每一台机器都是 派+硬盘 模式，派上放os，硬盘存储 存储设置：4T 与 操作系统分离 架构部署 IP 部件 OSD Monitor Managers MDSs 192.168.0.185 ceph-admin 192.168.0.134 ceph-client 192.168.0.130 ceph-server-1 Y Y 192.168.0.111 ceph-server-2 Y 192.168.0.105 ceph-server-3 Y Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/command/command.html":{"url":"docs/command/command.html","title":"常用命令","keywords":"","body":"Ceph M 快速安装 - 好脑袋和烂笔头 参考： https://my.oschina.net/guol/blog/1928348 有修改 ceph角色分配 172.31.68.241 admin-node/deph-deploy/mon/mgr/mds/rgw 172.31.68.242 osd.0/mon 172.31.68.243 osd.1/mon 配置ssh无密码登录 admin-node要可以无密码ssh登录osd机器，如果是普通用户，则要分配sudo权限，如下： useradd -d /home/cephadmin -m cephadmin echo \"cephadmin ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/cephadmin chmod 0440 /etc/sudoers.d/cephadmin 快速安装 ceph-deploy new ceph1 ceph-deploy install ceph1 ceph2 ceph3 ceph-deploy --overwrite-conf mon create-initial ceph-deploy admin ceph1 ceph2 ceph3 ceph-deploy mgr create ceph1 ceph-deploy osd create --data /dev/vdb1 ceph2 ceph-deploy osd create --data /dev/vdb1 ceph3 ceph health ceph -s 扩展 ceph-deploy mds create ceph1 ceph-deploy mon add ceph2 ceph-deploy mon add ceph3 ceph quorum_status --format json-pretty 本地配置上传 ceph-deploy --overwrite-conf config push ceph-admin ceph-server-1 ceph-server-2 安装rgw ceph-deploy rgw create ceph1 调整配置 [client.rgw.ceph1] rgw_frontends = \"civetweb port=8080\" ceph-deploy --overwrite-conf admin ceph1 ceph2 ceph3 ceph4 systemctl restart ceph-radosgw@rgw.ceph1.service curl http://ceph1:8080 -I 模拟client apt-get install ceph ceph-deploy admin ceph4 对象存储 echo 'hello ceph oject storage' > testfile.txt 创建pool ceph osd pool create mytest 8 上传文件 rados put test-object-1 testfile.txt --pool=mytest rados -p mytest ls 获取文件 rados get test-object-1 testfile.txt.1 --pool=mytest 查看映射位置 ceph osd map mytest test-object-1 删除文件 rados rm test-object-1 --pool=mytest 删除pool ceph osd pool rm mytest mytest --yes-i-really-really-mean-it 块存储 admin上执行：ceph osd pool create rdb 8 admin上执行：rbd pool init rdb admin上执行： rbd create foo --size 512 --image-feature layering -p rdb rbd map foo --name client.admin -p rdb cephfs admin：ceph osd pool create cephfs_data 4 admin：ceph osd pool create cephfs_metadata 4 admin：ceph osd lspools admin：ceph fs new cephfs cephfs_metadata cephfs_data admin.secret内容为ceph.client.admin.keyring内容的一部分 AQDhRX1baLeFFxAAskNapEuyipJ7SqS7Q1mh/Q== 内核级别挂载方法： mkdir /mnt/mycephfs mount -t ceph 172.31.68.241:6789,172.31.68.242:6789,172.31.68.243:6789:/ /mnt/mycephfs -o name=admin,secretfile=admin.secret 实验 cd /mnt/mycephfs echo 'hello ceph CephFS' > hello.txt cd ~ 卸载 umount -lf /mnt/mycephfs rm -rf /mnt/mycephfs 用户级别挂载： mkdir /mnt/mycephfs ceph-fuse -m 172.31.68.241:6789 /mnt/mycephfs S3 存储 ceph osd pool create .rgw 8 8 ceph osd pool create .rgw.root 8 8 ceph osd pool create .rgw.control 8 8 ceph osd pool create .rgw.gc 8 8 ceph osd pool create .rgw.buckets 8 8 ceph osd pool create .rgw.buckets.index 8 8 ceph osd pool create .rgw.buckets.extra 8 8 ceph osd pool create .log 8 8 ceph osd pool create .intent-log 8 8 ceph osd pool create .usage 8 8 ceph osd pool create .users 8 8 ceph osd pool create .users.email 8 8 ceph osd pool create .users.swift 8 8 ceph osd pool create .users.uid 8 8 vm外挂磁盘 cd /opt/vm/data_image qemu-img create -f qcow2 ubuntu16.04-2-data.img 2G qemu-img create -f qcow2 ubuntu16.04-3-data.img 2G virsh attach-disk [--domin] $DOMIN [--source] $SOURCEFILE [--target] $TARGET --subdriver qcow2 --config --live virsh attach-disk Ubuntu16.04-2 /opt/vm/data_image/ubuntu16.04-2-data.img vdb --subdriver qcow2 virsh attach-disk Ubuntu16.04-3 /opt/vm/data_image/ubuntu16.04-3-data.img vdb --subdriver qcow2 清除安装包 ceph-deploy purge ceph1 ceph2 ceph3 清除配置信息 ceph-deploy purgedata ceph1 ceph2 ceph3 ceph-deploy forgetkeys 每个节点删除残留的配置文件 rm -rf /var/lib/ceph/osd/* rm -rf /var/lib/ceph/mon/* rm -rf /var/lib/ceph/mds/* rm -rf /var/lib/ceph/bootstrap-mds/* rm -rf /var/lib/ceph/bootstrap-osd/* rm -rf /var/lib/ceph/bootstrap-mon/* rm -rf /var/lib/ceph/tmp/* rm -rf /etc/ceph/* rm -rf /var/run/ceph/* 清除lvm配置 vgscan vgdisplay -v lvremove vgremove pvremove ceph-deplop ceph-deploy disk list ceph-server-2 # 列出ceph-server-2节点磁盘 ref https://my.oschina.net/guol/blog/1928348 Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/installation/":{"url":"docs/installation/","title":"安装","keywords":"","body":"Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/installation/vagrant/carmstrong/multinode-ceph-vagrant.html":{"url":"docs/installation/vagrant/carmstrong/multinode-ceph-vagrant.html","title":"carmstrong/multinode-ceph-vagrant","keywords":"","body":"根据 https://github.com/carmstrong/multinode-ceph-vagrant 安装 ceph 开发环境 env os: windows 10 命令行： git-bash或者cmder或者cmd virtual: 6.0.4 vagrant: 2.2.3 vagrant box: ubuntu/bionic64 ceph：V12.2.11 LUMINOUS ceph-deploy：1.5.38 架构部署 部件 OSD Monitor Managers MDSs ceph-admin ceph-client ceph-server-1 Y Y ceph-server-2 Y ceph-server-3 Y clone repo git clone https://github.com/carmstrong/multinode-ceph-vagrant && cd multinode-ceph-vagrant vagrant plugin vagrant plugin install vagrant-cachier vagrant plugin install vagrant-hostmanager vagrant box add 提前下载好官方的 ubuntu/bionic64 vagrant box list vagrant box add ./ubuntu-18.04/bionic-server-cloudimg-amd64-vagrant.box --name ubuntu/bionic64 vagrant box list Add your Vagrant key to the SSH agent DELL@DESKTOP-MQ9CENU MINGW64 /f/tom/ceph/multinode-ceph-vagrant (master) $ ssh-add -k ~/.vagrant.d/insecure_private_key Could not open a connection to your authentication agent. DELL@DESKTOP-MQ9CENU MINGW64 /f/tom/ceph/multinode-ceph-vagrant (master) $ ssh-add -K ~/.vagrant.d/insecure_private_key Could not open a connection to your authentication agent. DELL@DESKTOP-MQ9CENU MINGW64 /f/tom/ceph/multinode-ceph-vagrant (master) $ 这里明显没有成功。 是不是因为我的环境中，之前有过 ~/.vagrant.d/insecure_private_key ? 我这里先跳过这一步 DELL@DESKTOP-MQ9CENU MINGW64 /f/tom/ceph/multinode-ceph-vagrant (master) $ ls ~/.vagrant.d/insecure_private_key /c/Users/DELL/.vagrant.d/insecure_private_key DELL@DESKTOP-MQ9CENU MINGW64 /f/tom/ceph/multinode-ceph-vagrant (master) $ 其实这一步，就是为了让后面的 vagrant ssh ceph-admin 等生效。 而 vagrant ssh ceph-admin 的原理，就是使用 ~/.vagrant.d/insecure_private_key 去登录 ceph-admin 中的 vagrant 用户。 你可以用这个原理，在 xshell 中导入 ~/.vagrant.d/insecure_private_key ，然后登录 ceph-admin，ceph-client，ceph-server-1 等机器 中的 vagrant 用户。 Start the VMs This instructs Vagrant to start the VMs and install ceph-deploy on the admin machine. $ vagrant up Create the cluster We'll create a simple cluster and make sure it's healthy. Then, we'll expand it. First, we need to get an interactive shell on the admin machine: $ vagrant ssh ceph-admin The ceph-deploy tool will write configuration files and logs to the current directory. So, let's create a directory for the new cluster: vagrant@ceph-admin:~$ mkdir test-cluster && cd test-cluster vagrant@ceph-admin:~/test-cluster$ ceph-deploy --version 1.5.38 vagrant@ceph-admin:~/test-cluster$ Let's prepare the machines: vagrant@ceph-admin:~/test-cluster$ ceph-deploy new ceph-server-1 ceph-server-2 ceph-server-3 这个时候可能会报错： vagrant@ceph-admin:~/test-cluster$ ceph-deploy new ceph-server-1 ceph-server-2 ceph-server-3 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/vagrant/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.38): /usr/bin/ceph-deploy new ceph-server-1 ceph-server-2 ceph-server-3 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] ssh_copykey : True [ceph_deploy.cli][INFO ] mon : ['ceph-server-1', 'ceph-server-2', 'ceph-server-3'] [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] public_network : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster_network : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] fsid : None [ceph_deploy.new][DEBUG ] Creating new cluster named ceph [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-server-1][DEBUG ] connected to host: ceph-admin [ceph-server-1][INFO ] Running command: ssh -CT -o BatchMode=yes ceph-server-1 [ceph_deploy.new][WARNIN] could not connect via SSH [ceph_deploy.new][INFO ] creating a passwordless id_rsa.pub key file [ceph_deploy.new][DEBUG ] connected to host: ceph-admin [ceph_deploy.new][INFO ] Running command: ssh-keygen -t rsa -N -f /home/vagrant/.ssh/id_rsa [ceph_deploy.new][DEBUG ] Generating public/private rsa key pair. [ceph_deploy.new][DEBUG ] Your identification has been saved in /home/vagrant/.ssh/id_rsa. [ceph_deploy.new][DEBUG ] Your public key has been saved in /home/vagrant/.ssh/id_rsa.pub. [ceph_deploy.new][DEBUG ] The key fingerprint is: [ceph_deploy.new][DEBUG ] SHA256:JXlwZhhC4h1ous7ywvgUEZLfqt6fUFVcS09NcTB17iI vagrant@ceph-admin [ceph_deploy.new][DEBUG ] The key's randomart image is: [ceph_deploy.new][DEBUG ] +---[RSA 2048]----+ [ceph_deploy.new][DEBUG ] |... .o+ ++=o .===| [ceph_deploy.new][DEBUG ] |.. ooo ooB. + +o| [ceph_deploy.new][DEBUG ] | ..+. ..o o. . .| [ceph_deploy.new][DEBUG ] | o.. . + . | [ceph_deploy.new][DEBUG ] | .o . S E . .| [ceph_deploy.new][DEBUG ] | o.. . . | [ceph_deploy.new][DEBUG ] |o+.. | [ceph_deploy.new][DEBUG ] |=o+ . . | [ceph_deploy.new][DEBUG ] |.*o..o | [ceph_deploy.new][DEBUG ] +----[SHA256]-----+ [ceph_deploy.new][INFO ] will connect again with password prompt The authenticity of host 'ceph-server-1 (172.21.12.12)' can't be established. ECDSA key fingerprint is SHA256:E9BwMCErur/NOG8wmkAUbl1j/q4gYtIKA2d1NBN0NQU. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'ceph-server-1,172.21.12.12' (ECDSA) to the list of known hosts. vagrant@ceph-server-1: Permission denied (publickey). [ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph-server-1 resulted in errors: HostNotFound ceph-server-1 vagrant@ceph-admin:~/test-cluster$ 在这种情况下，我先建立一个 snapshot DELL@DESKTOP-MQ9CENU MINGW64 /f/tom/ceph/multinode-ceph-vagrant (master) $ vagrant snapshot save ceph-admin init ==> ceph-admin: Snapshotting the machine as 'init'... ==> ceph-admin: Snapshot saved! You can restore the snapshot at any time by ==> ceph-admin: using `vagrant snapshot restore`. You can delete it using ==> ceph-admin: `vagrant snapshot delete`. DELL@DESKTOP-MQ9CENU MINGW64 /f/tom/ceph/multinode-ceph-vagrant (master) $ vagrant snapshot list ceph-admin init DELL@DESKTOP-MQ9CENU MINGW64 /f/tom/ceph/multinode-ceph-vagrant (master) $ vagrant snapshot save ceph-client init ==> ceph-client: Snapshotting the machine as 'init'... ==> ceph-client: Snapshot saved! You can restore the snapshot at any time by ==> ceph-client: using `vagrant snapshot restore`. You can delete it using ==> ceph-client: `vagrant snapshot delete`. DELL@DESKTOP-MQ9CENU MINGW64 /f/tom/ceph/multinode-ceph-vagrant (master) $ vagrant snapshot save ceph-server-1 init ==> ceph-server-1: Snapshotting the machine as 'init'... ==> ceph-server-1: Snapshot saved! You can restore the snapshot at any time by ==> ceph-server-1: using `vagrant snapshot restore`. You can delete it using ==> ceph-server-1: `vagrant snapshot delete`. DELL@DESKTOP-MQ9CENU MINGW64 /f/tom/ceph/multinode-ceph-vagrant (master) $ vagrant snapshot save ceph-server-2 init ==> ceph-server-2: Snapshotting the machine as 'init'... ==> ceph-server-2: Snapshot saved! You can restore the snapshot at any time by ==> ceph-server-2: using `vagrant snapshot restore`. You can delete it using ==> ceph-server-2: `vagrant snapshot delete`. DELL@DESKTOP-MQ9CENU MINGW64 /f/tom/ceph/multinode-ceph-vagrant (master) $ vagrant snapshot save ceph-server-3 init ==> ceph-server-3: Snapshotting the machine as 'init'... ==> ceph-server-3: Snapshot saved! You can restore the snapshot at any time by ==> ceph-server-3: using `vagrant snapshot restore`. You can delete it using ==> ceph-server-3: `vagrant snapshot delete`. 看一下之前的那个报错。应该就是从 ceph-admin 连接 ceph-server-1 的时候 出错了。 看过程，应该是 ceph-admin 自己建立了一个 id_rsa 然后，希望通过自己建立的这个 id_rsa 去登录 ceph-server-1 。 vagrant@ceph-admin:~/test-cluster$ ll ~/.ssh/ total 24 drwx------ 2 vagrant vagrant 4096 Feb 23 02:11 ./ drwxr-xr-x 6 vagrant vagrant 4096 Feb 23 02:11 ../ -rw-r--r-- 1 vagrant vagrant 409 Feb 19 16:00 authorized_keys -rw------- 1 vagrant vagrant 1679 Feb 23 02:11 id_rsa -rw-r--r-- 1 vagrant vagrant 400 Feb 23 02:11 id_rsa.pub -rw-r--r-- 1 vagrant vagrant 666 Feb 23 02:21 known_hosts vagrant@ceph-admin:~/test-cluster$ cat ~/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCtVkEjX/fomoDRPyBgGlmLX24yjXz83zxeB5Zb7Hmk2bpJNyEKnHmexuoCfxGkulQEtVGzsFxO4fdx1pLWZsJAkGIvAq7KZ41LLAeOCPSxB8CaGLGXdtpnDUgyGdGk88qOfWo+vCHqY2M7Vt3HKbtypb6lXsLBOsNiQStWavSPN1afipF9weuxn8nNvJZNKkHVbfzI6C0dVnFxzBTsEoyS1GBi1pTGuZyVmY8pYj7i47A3qVidUQuFIkQ9VZXIZ5RGuw/AaAvM4KaQ+qilu1NN0yCihHlQFLf8QtRvJou2JqYPDMHvNqQ1qDb056an1KOkoAvkAToHEnykyTNsb5Cn vagrant@ceph-admin vagrant@ceph-admin:~/test-cluster$ 那我们去 ceph-server-1 中去看一下，是否在 其下的 .ssh/authorized_keys 文件内容 vagrant@ceph-server-1:~$ ll .ssh/ total 12 drwx------ 2 vagrant vagrant 4096 Feb 19 16:00 ./ drwxr-xr-x 5 vagrant vagrant 4096 Feb 22 10:26 ../ -rw-r--r-- 1 vagrant vagrant 409 Feb 19 16:00 authorized_keys vagrant@ceph-server-1:~$ cat .ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key vagrant@ceph-server-1:~$ 很明显，ceph-server-1 的 /home/vagrant/.ssh/authorized_keys 文件内容中并没有 ceph-admin 的 /home/vagrant/.ssh/id_rsa.pub 的内容。这自然就无法登录。 那么问题出在哪里了？ 回头再看一下预检/CEPH 节点安装/允许无密码 SSH 登录 ，这需要 ceph-admin 的 允许无密码 SSH 登录，需要设置 /home/vagrant/.ssh/config 文件。 而 ceph-server-1 中没有这个 config 文件。 vagrant@ceph-server-1:~$ ls .ssh/ authorized_keys vagrant@ceph-server-1:~$ 那么我们手工完成一下吧。我这里通过写入各 ceph 节点 的 /home/vagrant/.ssh/authorized_keys 文件完成。下以 ceph-server-2 为例： vagrant@ceph-server-2:~$ echo \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCtVkEjX/fomoDRPyBgGlmLX24yjXz83zxeB5Zb7Hmk2bpJNyEKnHmexuoCfxGkulQEtVGzsFxO4fdx1pLWZsJAkGIvAq7KZ41LLAeOCPSxB8CaGLGXdtpnDUgyGdGk88qOfWo+vCHqY2M7Vt3HKbtypb6lXsLBOsNiQStWavSPN1afipF9weuxn8nNvJZNKkHVbfzI6C0dVnFxzBTsEoyS1GBi1pTGuZyVmY8pYj7i47A3qVidUQuFIkQ9VZXIZ5RGuw/AaAvM4KaQ+qilu1NN0yCihHlQFLf8QtRvJou2JqYPDMHvNqQ1qDb056an1KOkoAvkAToHEnykyTNsb5Cn vagrant@ceph-admin\" >> .ssh/authorized_keys 再ceph-deploy new ceph-server-1 ceph-server-2 ceph-server-3生成了 ceph.conf 和 ceph.mon.keyring vagrant@ceph-admin:~/test-cluster$ ceph-deploy new ceph-server-1 ceph-server-2 ceph-server-3 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/vagrant/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.38): /usr/bin/ceph-deploy new ceph-server-1 ceph-server-2 ceph-server-3 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] ssh_copykey : True [ceph_deploy.cli][INFO ] mon : ['ceph-server-1', 'ceph-server-2', 'ceph-server-3'] [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] public_network : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster_network : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] fsid : None [ceph_deploy.new][DEBUG ] Creating new cluster named ceph [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-server-1][DEBUG ] connected to host: ceph-admin [ceph-server-1][INFO ] Running command: ssh -CT -o BatchMode=yes ceph-server-1 [ceph-server-1][DEBUG ] connection detected need for sudo [ceph-server-1][DEBUG ] connected to host: ceph-server-1 [ceph-server-1][DEBUG ] detect platform information from remote host [ceph-server-1][DEBUG ] detect machine type [ceph-server-1][DEBUG ] find the location of an executable [ceph-server-1][INFO ] Running command: sudo /bin/ip link show [ceph-server-1][INFO ] Running command: sudo /bin/ip addr show [ceph-server-1][DEBUG ] IP addresses found: [u'172.21.12.12', u'10.0.2.15'] [ceph_deploy.new][DEBUG ] Resolving host ceph-server-1 [ceph_deploy.new][DEBUG ] Monitor ceph-server-1 at 172.21.12.12 [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-server-2][DEBUG ] connected to host: ceph-admin [ceph-server-2][INFO ] Running command: ssh -CT -o BatchMode=yes ceph-server-2 [ceph-server-2][DEBUG ] connection detected need for sudo [ceph-server-2][DEBUG ] connected to host: ceph-server-2 [ceph-server-2][DEBUG ] detect platform information from remote host [ceph-server-2][DEBUG ] detect machine type [ceph-server-2][DEBUG ] find the location of an executable [ceph-server-2][INFO ] Running command: sudo /bin/ip link show [ceph-server-2][INFO ] Running command: sudo /bin/ip addr show [ceph-server-2][DEBUG ] IP addresses found: [u'172.21.12.13', u'10.0.2.15'] [ceph_deploy.new][DEBUG ] Resolving host ceph-server-2 [ceph_deploy.new][DEBUG ] Monitor ceph-server-2 at 172.21.12.13 [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-server-3][DEBUG ] connected to host: ceph-admin [ceph-server-3][INFO ] Running command: ssh -CT -o BatchMode=yes ceph-server-3 [ceph-server-3][DEBUG ] connection detected need for sudo [ceph-server-3][DEBUG ] connected to host: ceph-server-3 [ceph-server-3][DEBUG ] detect platform information from remote host [ceph-server-3][DEBUG ] detect machine type [ceph-server-3][DEBUG ] find the location of an executable [ceph-server-3][INFO ] Running command: sudo /bin/ip link show [ceph-server-3][INFO ] Running command: sudo /bin/ip addr show [ceph-server-3][DEBUG ] IP addresses found: [u'10.0.2.15', u'172.21.12.14'] [ceph_deploy.new][DEBUG ] Resolving host ceph-server-3 [ceph_deploy.new][DEBUG ] Monitor ceph-server-3 at 172.21.12.14 [ceph_deploy.new][DEBUG ] Monitor initial members are ['ceph-server-1', 'ceph-server-2', 'ceph-server-3'] [ceph_deploy.new][DEBUG ] Monitor addrs are ['172.21.12.12', '172.21.12.13', '172.21.12.14'] [ceph_deploy.new][DEBUG ] Creating a random mon key... [ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring... [ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf... vagrant@ceph-admin:~/test-cluster$ ls ceph-deploy-ceph.log ceph.conf ceph.mon.keyring vagrant@ceph-admin:~/test-cluster$ Now, we have to change a default setting. For our initial cluster, we are only going to have two object storage daemons. We need to tell Ceph to allow us to achieve an active + clean state with just two Ceph OSDs. Add osd pool default size = 2 to ./ceph.conf. Because we're dealing with multiple VMs sharing the same host, we can expect to see more clock skew. We can tell Ceph that we'd like to tolerate slightly more clock skew by adding the following section to ceph.conf: mon_clock_drift_allowed = 1 After these few changes, the file should look similar to: [global] fsid = 7acac25d-2bd8-4911-807e-e35377e741bf mon_initial_members = ceph-server-1, ceph-server-2, ceph-server-3 mon_host = 172.21.12.12,172.21.12.13,172.21.12.14 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx osd pool default size = 2 mon_clock_drift_allowed = 1 实操作： vagrant@ceph-admin:~/test-cluster$ echo \"osd pool default size = 2\" >> ceph.conf vagrant@ceph-admin:~/test-cluster$ echo \"mon_clock_drift_allowed = 1\" >> ceph.conf vagrant@ceph-admin:~/test-cluster$ cat ceph.conf [global] fsid = c49c3fb1-cfb9-4f16-adfd-ba93b44bd3b5 mon_initial_members = ceph-server-1, ceph-server-2, ceph-server-3 mon_host = 172.21.12.12,172.21.12.13,172.21.12.14 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx osd pool default size = 2 mon_clock_drift_allowed = 1 vagrant@ceph-admin:~/test-cluster$ Install Ceph We're finally ready to install! Note here that we specify the Ceph release we'd like to install, which is luminous. vagrant@ceph-admin:~/test-cluster$ ceph-deploy install --release=luminous ceph-admin ceph-server-1 ceph-server-2 ceph-server-3 ceph-client 运行命令吧。日志放在 ./multinode-ceph-vagrant-install-log-part-2.log 和 ./multinode-ceph-vagrant-install-log-part-3.log 中 最后的输出如下： [ceph-client][DEBUG ] Created symlink /etc/systemd/system/multi-user.target.wants/ceph-mds.target → /lib/systemd/system/ceph-mds.target. [ceph-client][DEBUG ] Created symlink /etc/systemd/system/ceph.target.wants/ceph-mds.target → /lib/systemd/system/ceph-mds.target. [ceph-client][DEBUG ] Processing triggers for libc-bin (2.27-3ubuntu1) ... [ceph-client][DEBUG ] Processing triggers for ureadahead (0.100.0-20) ... [ceph-client][DEBUG ] Processing triggers for systemd (237-3ubuntu10.13) ... [ceph-client][INFO ] Running command: sudo ceph --version [ceph-client][DEBUG ] ceph version 12.2.8 (ae699615bac534ea496ee965ac6192cb7e0e07c0) luminous (stable) vagrant@ceph-admin:~/test-cluster$ ceph -v 一下 vagrant@ceph-admin:~/test-cluster$ ceph -v ceph version 12.2.8 (ae699615bac534ea496ee965ac6192cb7e0e07c0) luminous (stable) vagrant@ceph-admin:~/test-cluster$ Configure monitor and OSD services Next, we add a monitor node: vagrant@ceph-admin:~/test-cluster$ ceph-deploy mon create-initial 生成了几个新文件 vagrant@ceph-admin:~/test-cluster$ ls ceph-deploy-ceph.log ceph.bootstrap-mds.keyring ceph.bootstrap-mgr.keyring ceph.bootstrap-osd.keyring ceph.bootstrap-rgw.keyring ceph.client.admin.keyring ceph.conf ceph.mon.keyring vagrant@ceph-admin:~/test-cluster$ 具体日志在 ./multinode-ceph-vagrant-install-log-part-4.log 中 And our two OSDs. For these, we need to log into the server machines directly: vagrant@ceph-admin:~/test-cluster$ ssh ceph-server-2 \"sudo mkdir /var/local/osd0 && sudo chown ceph:ceph /var/local/osd0\" vagrant@ceph-admin:~/test-cluster$ ssh ceph-server-3 \"sudo mkdir /var/local/osd1 && sudo chown ceph:ceph /var/local/osd1\" Now we can prepare and activate the OSDs: vagrant@ceph-admin:~/test-cluster$ ceph-deploy osd prepare ceph-server-2:/var/local/osd0 ceph-server-3:/var/local/osd1 vagrant@ceph-admin:~/test-cluster$ ceph-deploy osd activate ceph-server-2:/var/local/osd0 ceph-server-3:/var/local/osd1 具体日志在 `./multinode-ceph-vagrant-install-log-part-5.log 中 Configuration and status We can copy our config file and admin key to all the nodes, so each one can use the ceph CLI. vagrant@ceph-admin:~/test-cluster$ ceph-deploy admin ceph-admin ceph-server-1 ceph-server-2 ceph-server-3 ceph-client 具体如下： vagrant@ceph-admin:~/test-cluster$ ceph-deploy admin ceph-admin ceph-server-1 ceph-server-2 ceph-server-3 ceph-client [ceph_deploy.conf][DEBUG ] found configuration file at: /home/vagrant/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.38): /usr/bin/ceph-deploy admin ceph-admin ceph-server-1 ceph-server-2 ceph-server-3 ceph-client [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] client : ['ceph-admin', 'ceph-server-1', 'ceph-server-2', 'ceph-server-3', 'ceph-client'] [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-admin [ceph-admin][DEBUG ] connection detected need for sudo [ceph-admin][DEBUG ] connected to host: ceph-admin [ceph-admin][DEBUG ] detect platform information from remote host [ceph-admin][DEBUG ] detect machine type [ceph-admin][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-server-1 [ceph-server-1][DEBUG ] connection detected need for sudo [ceph-server-1][DEBUG ] connected to host: ceph-server-1 [ceph-server-1][DEBUG ] detect platform information from remote host [ceph-server-1][DEBUG ] detect machine type [ceph-server-1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-server-2 [ceph-server-2][DEBUG ] connection detected need for sudo [ceph-server-2][DEBUG ] connected to host: ceph-server-2 [ceph-server-2][DEBUG ] detect platform information from remote host [ceph-server-2][DEBUG ] detect machine type [ceph-server-2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-server-3 [ceph-server-3][DEBUG ] connection detected need for sudo [ceph-server-3][DEBUG ] connected to host: ceph-server-3 [ceph-server-3][DEBUG ] detect platform information from remote host [ceph-server-3][DEBUG ] detect machine type [ceph-server-3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf [ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-client [ceph-client][DEBUG ] connection detected need for sudo [ceph-client][DEBUG ] connected to host: ceph-client [ceph-client][DEBUG ] detect platform information from remote host [ceph-client][DEBUG ] detect machine type [ceph-client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf vagrant@ceph-admin:~/test-cluster$ ls ceph-deploy-ceph.log ceph.bootstrap-mds.keyring ceph.bootstrap-mgr.keyring ceph.bootstrap-osd.keyring ceph.bootstrap-rgw.keyring ceph.client.admin.keyring ceph.conf ceph.mon.keyring vagrant@ceph-admin:~/test-cluster$ ls /etc/ceph/ -l total 12 -rw------- 1 root root 63 Feb 23 03:43 ceph.client.admin.keyring -rw-r--r-- 1 root root 313 Feb 23 03:43 ceph.conf -rw-r--r-- 1 root root 92 Jan 29 09:32 rbdmap -rw------- 1 root root 0 Feb 23 03:43 tmpXyXDGy vagrant@ceph-admin:~/test-cluster$ date Sat Feb 23 03:43:31 UTC 2019 vagrant@ceph-admin:~/test-cluster$ ls ceph-deploy-ceph.log ceph.bootstrap-mds.keyring ceph.bootstrap-mgr.keyring ceph.bootstrap-osd.keyring ceph.bootstrap-rgw.keyring ceph.client.admin.keyring ceph.conf ceph.mon.keyring vagrant@ceph-admin:~/test-cluster$ 看 /etc/ceph/ 文件时间，知道，是刚刚写入的。 然后去 ceph-server-3 中检查一下。 vagrant@ceph-server-3:~$ ceph -v ceph version 12.2.8 (ae699615bac534ea496ee965ac6192cb7e0e07c0) luminous (stable) vagrant@ceph-server-3:~$ ls /etc/ceph/ -l total 12 -rw------- 1 root root 63 Feb 23 03:43 ceph.client.admin.keyring -rw-r--r-- 1 root root 313 Feb 23 03:43 ceph.conf -rw-r--r-- 1 root root 92 Jan 29 09:32 rbdmap -rw------- 1 root root 0 Feb 23 03:34 tmpADlUqE vagrant@ceph-server-3:~$ date Sat Feb 23 03:43:48 UTC 2019 vagrant@ceph-server-3:~$ 没毛病。 We also should make sure the keyring is readable: vagrant@ceph-admin:~/test-cluster$ sudo chmod +r /etc/ceph/ceph.client.admin.keyring vagrant@ceph-admin:~/test-cluster$ ssh ceph-server-1 sudo chmod +r /etc/ceph/ceph.client.admin.keyring vagrant@ceph-admin:~/test-cluster$ ssh ceph-server-2 sudo chmod +r /etc/ceph/ceph.client.admin.keyring vagrant@ceph-admin:~/test-cluster$ ssh ceph-server-3 sudo chmod +r /etc/ceph/ceph.client.admin.keyring Finally, check on the health of the cluster: 我的与文档的不同。 文档的： vagrant@ceph-admin:~/test-cluster$ ceph health You should see something similar to this once it's healthy: vagrant@ceph-admin:~/test-cluster$ ceph health HEALTH_OK vagrant@ceph-admin:~/test-cluster$ ceph -s cluster 18197927-3d77-4064-b9be-bba972b00750 health HEALTH_OK monmap e2: 3 mons at {ceph-server-1=172.21.12.12:6789/0,ceph-server-2=172.21.12.13:6789/0,ceph-server-3=172.21.12.14:6789/0}, election epoch 6, quorum 0,1,2 ceph-server-1,ceph-server-2,ceph-server-3 osdmap e9: 2 osds: 2 up, 2 in pgmap v13: 192 pgs, 3 pools, 0 bytes data, 0 objects 12485 MB used, 64692 MB / 80568 MB avail 192 active+clean Notice that we have two OSDs (osdmap e9: 2 osds: 2 up, 2 in) and all of the placement groups (pgs) are reporting as active+clean. 我的： vagrant@ceph-admin:~/test-cluster$ ceph health HEALTH_WARN no active mgr vagrant@ceph-admin:~/test-cluster$ ceph -s cluster: id: c49c3fb1-cfb9-4f16-adfd-ba93b44bd3b5 health: HEALTH_WARN no active mgr services: mon: 3 daemons, quorum ceph-server-1,ceph-server-2,ceph-server-3 mgr: no daemons active osd: 2 osds: 2 up, 2 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0B usage: 0B used, 0B / 0B avail pgs: vagrant@ceph-admin:~/test-cluster$ 说明我们没有安装 mgr , 那安装一下： 安装在 ceph-server-1 . vagrant@ceph-admin:~/test-cluster$ ceph-deploy mgr create ceph-server-1 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/vagrant/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (1.5.38): /usr/bin/ceph-deploy mgr create ceph-server-1 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] mgr : [('ceph-server-1', 'ceph-server-1')] [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] subcommand : create [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.mgr][DEBUG ] Deploying mgr, cluster ceph hosts ceph-server-1:ceph-server-1 [ceph-server-1][DEBUG ] connection detected need for sudo [ceph-server-1][DEBUG ] connected to host: ceph-server-1 [ceph-server-1][DEBUG ] detect platform information from remote host [ceph-server-1][DEBUG ] detect machine type [ceph_deploy.mgr][INFO ] Distro info: Ubuntu 18.04 bionic [ceph_deploy.mgr][DEBUG ] remote host will use systemd [ceph_deploy.mgr][DEBUG ] deploying mgr bootstrap to ceph-server-1 [ceph-server-1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf [ceph-server-1][WARNIN] mgr keyring does not exist yet, creating one [ceph-server-1][DEBUG ] create a keyring file [ceph-server-1][DEBUG ] create path if it doesn't exist [ceph-server-1][INFO ] Running command: sudo ceph --cluster ceph --name client.bootstrap-mgr --keyring /var/lib/ceph/bootstrap-mgr/ceph.keyring auth get-or-create mgr.ceph-server-1 mon allow profile mgr osd allow * mds allow * -o /var/lib/ceph/mgr/ceph-ceph-server-1/keyring [ceph-server-1][INFO ] Running command: sudo systemctl enable ceph-mgr@ceph-server-1 [ceph-server-1][WARNIN] Created symlink /etc/systemd/system/ceph-mgr.target.wants/ceph-mgr@ceph-server-1.service → /lib/systemd/system/ceph-mgr@.service. [ceph-server-1][INFO ] Running command: sudo systemctl start ceph-mgr@ceph-server-1 [ceph-server-1][INFO ] Running command: sudo systemctl enable ceph.target vagrant@ceph-admin:~/test-cluster$ ceph -s cluster: id: c49c3fb1-cfb9-4f16-adfd-ba93b44bd3b5 health: HEALTH_OK services: mon: 3 daemons, quorum ceph-server-1,ceph-server-2,ceph-server-3 mgr: ceph-server-1(active) osd: 2 osds: 2 up, 2 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0B usage: 2.00GiB used, 18.0GiB / 20GiB avail pgs: vagrant@ceph-admin:~/test-cluster$ Congratulations! 好了，到这里，我们一个基本的ceph环境的已经搭建好了！ 这个时候，最好快照一下。 $ vagrant snapshot save ceph-admin basic $ vagrant snapshot save ceph-client basic $ vagrant snapshot save ceph-server-1 basic $ vagrant snapshot save ceph-server-2 basic $ vagrant snapshot save ceph-server-3 basic Expanding the cluster （没有实践） To more closely model a production cluster, we're going to add one more OSD daemon and a Ceph Metadata Server. We'll also add monitors to all hosts instead of just one. Add an OSD vagrant@ceph-admin:~/test-cluster$ ssh ceph-server-1 \"sudo mkdir /var/local/osd2 && sudo chown ceph:ceph /var/local/osd2\" Now, from the admin node, we prepare and activate the OSD: vagrant@ceph-admin:~/test-cluster$ ceph-deploy osd prepare ceph-server-1:/var/local/osd2 vagrant@ceph-admin:~/test-cluster$ ceph-deploy osd activate ceph-server-1:/var/local/osd2 Watch the rebalancing: vagrant@ceph-admin:~/test-cluster$ ceph -w You should eventually see it return to an active+clean state, but this time with 3 OSDs: vagrant@ceph-admin:~/test-cluster$ ceph -w cluster 18197927-3d77-4064-b9be-bba972b00750 health HEALTH_OK monmap e2: 3 mons at {ceph-server-1=172.21.12.12:6789/0,ceph-server-2=172.21.12.13:6789/0,ceph-server-3=172.21.12.14:6789/0}, election epoch 30, quorum 0,1,2 ceph-server-1,ceph-server-2,ceph-server-3 osdmap e38: 3 osds: 3 up, 3 in pgmap v415: 192 pgs, 3 pools, 0 bytes data, 0 objects 18752 MB used, 97014 MB / 118 GB avail 192 active+clean Add metadata server Let's add a metadata server to server1: vagrant@ceph-admin:~/test-cluster$ ceph-deploy mds create ceph-server-1 Add more monitors（没有实践） We add monitors to servers 2 and 3. vagrant@ceph-admin:~/test-cluster$ ceph-deploy mon create ceph-server-2 ceph-server-3 Watch the quorum status, and ensure it's happy: vagrant@ceph-admin:~/test-cluster$ ceph quorum_status --format json-pretty Install Ceph Object Gateway（没有实践） TODO Play around! Now that we have everything set up, let's actually use the cluster. We'll use the ceph-client machine for this. Create a block device $ vagrant ssh ceph-client vagrant@ceph-client:~$ sudo rbd create foo --size 4096 -m ceph-server-1 vagrant@ceph-client:~$ sudo rbd map foo --pool rbd --name client.admin -m ceph-server-1 vagrant@ceph-client:~$ sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo vagrant@ceph-client:~$ sudo mkdir /mnt/ceph-block-device vagrant@ceph-client:~$ sudo mount /dev/rbd/rbd/foo /mnt/ceph-block-device Create a mount with Ceph FS TODO Store a blob object TODO Cleanup（没有实践） When you're all done, tell Vagrant to destroy the VMs. $ vagrant destroy -f Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/installation/vagrant/oprypin/vagrant-ceph-quickstart.html":{"url":"docs/installation/vagrant/oprypin/vagrant-ceph-quickstart.html","title":"oprypin/vagrant-ceph-quickstart","keywords":"","body":"https://github.com/oprypin/vagrant-ceph-quickstart 这个使用的box是 debian/jessie64 。所以使用 debian 的朋友可以试一下。我没试。 Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/installation/quick-install.html":{"url":"docs/installation/quick-install.html","title":"quick install","keywords":"","body":"本文是快速安装 根据 http://docs.ceph.com/docs/master/start/ 安装 ceph 开发环境 env 软件 os：Ubuntu Server 16.04×64 Python: 2.7.12 ceph：v13.2.4 Mimic ceph-deploy：2.0.1 硬件 每一台机器都是 派+硬盘 模式，派上放os，硬盘存储 存储设置：4T 与 操作系统分离 架构部署 IP 部件 OSD Monitor Managers MDSs 192.168.0.185 ceph-admin 192.168.0.134 ceph-client 192.168.0.130 ceph-server-1 Y Y 192.168.0.111 ceph-server-2 Y 192.168.0.105 ceph-server-3 Y step 除非说明，则均为所有节点 etc-hosts 配置 /etc/hosts 文件 192.168.0.185 ceph-admin 192.168.0.134 ceph-client 192.168.0.130 ceph-server-1 192.168.0.111 ceph-server-2 192.168.0.105 ceph-server-3 hostnamectl --static set-hostname *** 设置 hostname ceph-admin ceph-client ceph-server-1 ceph-server-2 ceph-server-3 ntp sudo apt-get update && sudo apt-get install -yq ntp openssh-server 创建 ceph 用户 ssh user@ceph-server sudo useradd -d /home/{username} -m {username} sudo passwd {username} echo \"{username} ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/{username} sudo chmod 0440 /etc/sudoers.d/{username} 因为我这里的环境已经有 admin 用户，则把 admin 作为 ceph 用户 echo \"admin ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/admin sudo chmod 0440 /etc/sudoers.d/admin ceph-deploy （ceph-admin节点） wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add - echo deb https://download.ceph.com/debian-mimic/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list sudo apt-get update && sudo apt-get install -yq ceph-deploy ceph-deploy --version ENABLE PASSWORD-LESS SSH（ceph-admin节点） ssh-keygen Copy the key to each Ceph Node （ceph-admin节点） ssh-copy-id {username}@node1 ssh-copy-id {username}@node2 ssh-copy-id {username}@node3 检查一下，是否能免密码登录 OPEN REQUIRED PORTS 如果是实验环境，这一步可暂时忽略。 如果是由 firewall-cmd 管理的话： For example, on monitors: sudo firewall-cmd --zone=public --add-service=ceph-mon --permanent and on OSDs and MDSs: sudo firewall-cmd --zone=public --add-service=ceph --permanent Once you have finished configuring firewalld with the --permanent flag, you can make the changes live immediately without rebooting: sudo firewall-cmd --reload CREATE A CLUSTER （ceph-admin节点） ceph-deploy install （ceph-admin节点） ceph-deploy new ceph-server-1 ceph-server-2 ceph-server-3 echo \"osd pool default size = 2\" >> ceph.conf echo \"mon_clock_drift_allowed = 1\" >> ceph.conf cat ceph.conf ceph-deploy install ceph-admin ceph-server-1 ceph-server-2 ceph-server-3 ceph-client ceph -v Configure monitor and OSD services（ceph-admin节点） ceph-deploy mon create-initial ls Configuration and status（ceph-admin节点） ceph-deploy admin ceph-admin ceph-server-1 ceph-server-2 ceph-server-3 ceph-client ceph -v ls /etc/ceph/ -l sudo chmod +r /etc/ceph/ceph.client.admin.keyring ssh ceph-server-1 sudo chmod +r /etc/ceph/ceph.client.admin.keyring ssh ceph-server-2 sudo chmod +r /etc/ceph/ceph.client.admin.keyring ssh ceph-server-3 sudo chmod +r /etc/ceph/ceph.client.admin.keyring Configure mgr （ceph-admin节点） ceph-deploy mgr create ceph-server-1 ceph health （除 ceph-client 节点） ceph health ceph -s ref http://docs.ceph.com/docs/master/start/ Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/installation/add-osds-with-quick-install.html":{"url":"docs/installation/add-osds-with-quick-install.html","title":"添加osd","keywords":"","body":"本文基于 ceph deploy quick install 而 Add OSDs env 同上 step Add OSDs admin@lattepanda:~$ ceph health HEALTH_OK admin@lattepanda:~$ ceph -s cluster: id: 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 health: HEALTH_OK services: mon: 3 daemons, quorum ceph-server-3,ceph-server-2,ceph-server-1 mgr: ceph-server-1(active) osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: admin@lattepanda:~$ 大家看到，现在的 osd , pools, pgs, objects 等数据均为0。 我们要把 硬盘加上去，才算是把 osd 分配完成。 ceph-deploy osd create --data /dev/sda1 ceph-server-2 ceph-deploy osd create --data /dev/sda1 ceph-server-3 报错GenericError: Failed to create 1 OSDs 如果 Failed to execute command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/sda1，具体如下： admin@ceph-admin:~/test-cluster$ ceph-deploy osd create --data /dev/sda1 ceph-server-3 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/admin/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy osd create --data /dev/sda1 ceph-server-3 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] bluestore : None [ceph_deploy.cli][INFO ] cd_conf : [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] fs_type : xfs [ceph_deploy.cli][INFO ] block_wal : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] journal : None [ceph_deploy.cli][INFO ] subcommand : create [ceph_deploy.cli][INFO ] host : ceph-server-3 [ceph_deploy.cli][INFO ] filestore : None [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] zap_disk : False [ceph_deploy.cli][INFO ] data : /dev/sda1 [ceph_deploy.cli][INFO ] block_db : None [ceph_deploy.cli][INFO ] dmcrypt : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] dmcrypt_key_dir : /etc/ceph/dmcrypt-keys [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] debug : False [ceph_deploy.osd][DEBUG ] Creating OSD on cluster ceph with data device /dev/sda1 [ceph-server-3][DEBUG ] connection detected need for sudo [ceph-server-3][DEBUG ] connected to host: ceph-server-3 [ceph-server-3][DEBUG ] detect platform information from remote host [ceph-server-3][DEBUG ] detect machine type [ceph-server-3][DEBUG ] find the location of an executable [ceph_deploy.osd][INFO ] Distro info: Ubuntu 16.04 xenial [ceph_deploy.osd][DEBUG ] Deploying osd to ceph-server-3 [ceph-server-3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf [ceph-server-3][DEBUG ] find the location of an executable [ceph-server-3][INFO ] Running command: sudo /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/sda1 [ceph-server-3][WARNIN] --> RuntimeError: command returned non-zero exit status: 5 [ceph-server-3][DEBUG ] Running command: /usr/bin/ceph-authtool --gen-print-key [ceph-server-3][DEBUG ] Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new f2d1197f-301b-419a-a83b-e5cb8634e594 [ceph-server-3][DEBUG ] Running command: /sbin/vgcreate --force --yes ceph-d702b141-e07b-4067-ac55-f12f2ac7fd90 /dev/sda1 [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: /run/lvm/lvmetad.socket: connect failed: No such file or directory [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: WARNING: Failed to connect to lvmetad. Falling back to internal scanning. [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: /dev/mmcblk0rpmb: read failed after 0 of 4096 at 4128768: Input/output error [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: /dev/mmcblk0rpmb: read failed after 0 of 4096 at 4186112: Input/output error [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: /dev/mmcblk0rpmb: read failed after 0 of 4096 at 0: Input/output error [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: /dev/mmcblk0rpmb: read failed after 0 of 4096 at 4096: Input/output error [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: Can't open /dev/sda1 exclusively. Mounted filesystem? [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] stderr: Unable to add physical volume '/dev/sda1' to volume group 'ceph-d702b141-e07b-4067-ac55-f12f2ac7fd90'. [ceph-server-3][DEBUG ] stderr: [ceph-server-3][DEBUG ] --> Was unable to complete a new OSD, will rollback changes [ceph-server-3][DEBUG ] Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd purge-new osd.1 --yes-i-really-mean-it [ceph-server-3][DEBUG ] stderr: purged osd.1 [ceph-server-3][DEBUG ] stderr: [ceph-server-3][ERROR ] RuntimeError: command returned non-zero exit status: 1 [ceph_deploy.osd][ERROR ] Failed to execute command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data /dev/sda1 [ceph_deploy][ERROR ] GenericError: Failed to create 1 OSDs admin@ceph-admin:~/test-cluster$ 可能原因： 硬盘未处于 umount 状态。比如已经mount了。 admin@ceph-server-3:~$ mount |grep \"sda1\" /dev/sda1 on /media/admin/f7d6f660-f0cf-4c92-8cef-abc8db2d0d0a type xfs (rw,nosuid,nodev,relatime,attr2,inode64,noquota,uhelper=udisks2) admin@ceph-server-3:~$ umount /media/admin/f7d6f660-f0cf-4c92-8cef-abc8db2d0d0a ceph -s admin@ceph-admin:~/test-cluster$ ceph -s cluster: id: 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 health: HEALTH_OK services: mon: 3 daemons, quorum ceph-server-3,ceph-server-2,ceph-server-1 mgr: ceph-server-1(active) osd: 2 osds: 2 up, 2 in rgw: 1 daemon active data: pools: 5 pools, 40 pgs objects: 187 objects, 1.1 KiB usage: 2.0 GiB used, 7.3 TiB / 7.3 TiB avail pgs: 40 active+clean admin@ceph-admin:~/test-cluster$ 这里，会显示出 osd, pools, objects 的数量，且 pgs 为 active+clean objects 在未存放数据的情况下不为0 Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/installation/add-an-rgw-instance-with-quick-install.html":{"url":"docs/installation/add-an-rgw-instance-with-quick-install.html","title":"添加rgw实例","keywords":"","body":"在 ceph deploy quick install 基础上安装一个 RGW INSTANCE env 软件 os：Ubuntu Server 16.04×64 Python: 2.7.12 ceph：v13.2.4 Mimic ceph-deploy：2.0.1 硬件 每一台机器都是 派+硬盘 模式，派上放os，硬盘存储 存储设置：4T 与 操作系统分离 架构部署 IP 部件 OSD Monitor Managers MDSs 192.168.0.185 ceph-admin 192.168.0.134 ceph-client 192.168.0.130 ceph-server-1 Y Y 192.168.0.111 ceph-server-2 Y 192.168.0.105 ceph-server-3 Y RGW 安装 在 ceph-client 上。 step 查看ceph 状态 admin@ceph-admin:~/test-cluster$ ceph health HEALTH_OK admin@ceph-admin:~/test-cluster$ admin@ceph-admin:~/test-cluster$ ceph -s cluster: id: 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 health: HEALTH_OK services: mon: 3 daemons, quorum ceph-server-3,ceph-server-2,ceph-server-1 mgr: ceph-server-1(active) osd: 2 osds: 2 up, 2 in rgw: 1 daemon active data: pools: 5 pools, 40 pgs objects: 187 objects, 1.1 KiB usage: 2.0 GiB used, 7.3 TiB / 7.3 TiB avail pgs: 40 active+clean admin@ceph-admin:~/test-cluster$ （此步看需求，可忽略）修改RGW访问端口7480为8080 RGW开始运，我们就可以通过端口7480（如果没有修改的话）来访问。如果你希望修改成8080的话，往下看。 修改配置 admin@ceph-admin:~/test-cluster$ vi ceph.conf 添加如下内容, 其中ceph-client是你希望添加RGW的节点名，8080为将来访问port。 [client.rgw.ceph-client] rgw_frontends = \"civetweb port=8080\" 分发配置文件 ceph-deploy --overwrite-conf config push ceph-admin ceph-client ceph-server-1 ceph-server-2 ceph-server-3 安装CEPH OBJECT GATEWAY ceph-deploy install --rgw ceph-client 管理RGW节点 Ceph CLI工具需要在管理员模式下运行，因此需要执行以下命令： ceph-deploy admin ceph-client 安装RGW实例 执行命令： ceph-deploy rgw create ceph-client 一旦RGW开始运行，我们就可以通过端口7480（如果没有修改的话）来访问。如： http://ceph-client:7480 如果RGW运行正常，它应该返回类似的信息： anonymous 我的实操： admin@ceph-admin:~/test-cluster$ curl http://ceph-client:8080 anonymousadmin@ceph-admin:~/test-cluster$ admin@ceph-admin:~/test-cluster$ curl http://ceph-client:8080 -I HTTP/1.1 200 OK x-amz-request-id: tx000000000000000000003-005c77a1a5-10c0-default Content-Type: application/xml Content-Length: 0 Date: Thu, 28 Feb 2019 08:53:57 GMT admin@ceph-admin:~/test-cluster$ 注意：剩下的创建用户的步骤都应该在RGW节点上运行。 对象存储 操作 echo 'hello ceph oject storage' > testfile.txt 创建pool ceph osd pool create mytest 8 上传文件 rados put test-object-1 testfile.txt --pool=mytest rados -p mytest ls 获取文件 rados get test-object-1 testfile.txt.1 --pool=mytest 查看映射位置 ceph osd map mytest test-object-1 删除文件 rados rm test-object-1 --pool=mytest 删除pool ceph osd pool rm mytest mytest --yes-i-really-really-mean-it 实操： admin@ceph-admin:~/test-cluster$ mkdir ~/test-data/ && cd ~/test-data/ admin@ceph-admin:~/test-cluster$ echo \"this is my first test data\" > testfile.txt admin@ceph-admin:~/test-data$ ceph osd pool create mytest 8 admin@ceph-admin:~/test-data$ rados put test-object-1 ./testfile.txt --pool=mytest admin@ceph-admin:~/test-cluster$ rados -p mytest ls test-object-1 admin@ceph-admin:~/test-cluster$ ceph osd map mytest test-object-1 osdmap e26 pool 'mytest' (2) object 'test-object-1' -> pg 2.74dc35e2 (2.2) -> up ([1,0], p1) acting ([1,0], p1) admin@ceph-admin:~/test-cluster$ 这样，就完成了 RGW 的基本操作。 (忽略)ceph osd pool create mytest 8 后 此时，ceph -s 会发现 在 data/gps 中会多出 pgs: 100.000% pgs unknown 8 unknown 这样的内容。 查看状态 admin@ceph-admin:~/test-data$ ceph -s cluster: id: 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 health: HEALTH_WARN application not enabled on 1 pool(s) services: mon: 3 daemons, quorum ceph-server-3,ceph-server-2,ceph-server-1 mgr: ceph-server-1(active) osd: 2 osds: 2 up, 2 in rgw: 1 daemon active data: pools: 5 pools, 40 pgs objects: 188 objects, 1.2 KiB usage: 2.0 GiB used, 7.3 TiB / 7.3 TiB avail pgs: 40 active+clean io: client: 85 B/s wr, 0 op/s rd, 0 op/s wr admin@ceph-admin:~/test-cluster$ 记录一下配置文件 admin@ceph-admin:~/test-cluster$ ls ceph.bootstrap-mds.keyring ceph.bootstrap-mgr.keyring ceph.bootstrap-osd.keyring ceph.bootstrap-rgw.keyring ceph.client.admin.keyring ceph.conf ceph-deploy-ceph.log ceph.mon.keyring release.asc admin@ceph-admin:~/test-cluster$ cat ceph.conf [global] fsid = 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 mon_initial_members = ceph-server-1, ceph-server-2, ceph-server-3 mon_host = 192.168.0.130,192.168.0.111,192.168.0.105 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx osd pool default size = 2 mon_clock_drift_allowed = 1 [client.rgw.ceph-client] rgw_frontends = \"civetweb port=8080\" admin@ceph-admin:~/test-cluster$ ls /etc/ceph/ ceph.client.admin.keyring ceph.conf rbdmap tmp4FMEG8 tmpEO0ZBg admin@ceph-admin:~/test-cluster$ 如果 RGW 安装过程中出了问题 通过netstat查看 RGW 配置的端口，判断rgw服务是否启动 admin@swift0134:~$ netstat -tnlp | grep 8080 (Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.) tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN - admin@swift0134:~$ sudo systemctl restart ceph-radosgw@rgw.ceph-client.service 查看rgw服务状态。sudo systemctl status ceph-radosgw@rgw.ceph-client.service admin@swift0134:~$ sudo systemctl status ceph-radosgw@rgw.ceph-client.service admin@swift0134:~$ sudo systemctl restart ceph-radosgw@rgw.ceph-client.service ref http://docs.ceph.com/docs/master/start/quick-ceph-deploy/ https://blog.csdn.net/styshoo/article/details/58572816 https://my.oschina.net/guol/blog/1928348 Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/installation/install-faq.html":{"url":"docs/installation/install-faq.html","title":"Installation 常见报错","keywords":"","body":"ceph-deploy 报 sudo: no tty present and no askpass program specified 说明，ceph用户没有能力在 ssh ceph-client \"sudo ls\" 时不输入密码。 解决：各节点运行 echo \"admin ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/admin sudo chmod 0440 /etc/sudoers.d/admin 报错日志： admin@ceph-admin:~$ ceph-deploy -v new ceph-server-1 ceph-server-2 ceph-server-3 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/admin/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy -v new ceph-server-1 ceph-server-2 ceph-server-3 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : True [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] ssh_copykey : True [ceph_deploy.cli][INFO ] mon : ['ceph-server-1', 'ceph-server-2', 'ceph-server-3'] [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] public_network : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster_network : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] fsid : None [ceph_deploy.new][DEBUG ] Creating new cluster named ceph [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-server-1][DEBUG ] connected to host: ceph-admin [ceph-server-1][INFO ] Running command: ssh -CT -o BatchMode=yes ceph-server-1 [ceph-server-1][DEBUG ] connection detected need for sudo sudo: no tty present and no askpass program specified [ceph_deploy][ERROR ] RuntimeError: connecting to host: ceph-server-1 resulted in errors: IOError cannot send (already closed?) admin@ceph-admin:~$ admin@ceph-admin:~$ echo \"admin ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/admin [sudo] password for admin: admin ALL = (root) NOPASSWD:ALL admin@ceph-admin:~$ sudo chmod 0440 /etc/sudoers.d/admin admin@ceph-admin:~$ ceph-deploy new ceph-server-1 ceph-server-2 ceph-server-3 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/admin/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy new ceph-server-1 ceph-server-2 ceph-server-3 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] ssh_copykey : True [ceph_deploy.cli][INFO ] mon : ['ceph-server-1', 'ceph-server-2', 'ceph-server-3'] [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] public_network : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] cluster_network : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] fsid : None [ceph_deploy.new][DEBUG ] Creating new cluster named ceph [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-server-1][DEBUG ] connected to host: ceph-admin [ceph-server-1][INFO ] Running command: ssh -CT -o BatchMode=yes ceph-server-1 [ceph-server-1][DEBUG ] connection detected need for sudo [ceph-server-1][DEBUG ] connected to host: ceph-server-1 [ceph-server-1][DEBUG ] detect platform information from remote host [ceph-server-1][DEBUG ] detect machine type [ceph-server-1][DEBUG ] find the location of an executable [ceph-server-1][INFO ] Running command: sudo /bin/ip link show [ceph-server-1][INFO ] Running command: sudo /bin/ip addr show [ceph-server-1][DEBUG ] IP addresses found: [u'192.168.0.130'] [ceph_deploy.new][DEBUG ] Resolving host ceph-server-1 [ceph_deploy.new][DEBUG ] Monitor ceph-server-1 at 192.168.0.130 [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-server-2][DEBUG ] connected to host: ceph-admin [ceph-server-2][INFO ] Running command: ssh -CT -o BatchMode=yes ceph-server-2 [ceph-server-2][DEBUG ] connection detected need for sudo [ceph-server-2][DEBUG ] connected to host: ceph-server-2 [ceph-server-2][DEBUG ] detect platform information from remote host [ceph-server-2][DEBUG ] detect machine type [ceph-server-2][DEBUG ] find the location of an executable [ceph-server-2][INFO ] Running command: sudo /bin/ip link show [ceph-server-2][INFO ] Running command: sudo /bin/ip addr show [ceph-server-2][DEBUG ] IP addresses found: [u'192.168.0.111'] [ceph_deploy.new][DEBUG ] Resolving host ceph-server-2 [ceph_deploy.new][DEBUG ] Monitor ceph-server-2 at 192.168.0.111 [ceph_deploy.new][INFO ] making sure passwordless SSH succeeds [ceph-server-3][DEBUG ] connected to host: ceph-admin [ceph-server-3][INFO ] Running command: ssh -CT -o BatchMode=yes ceph-server-3 [ceph-server-3][DEBUG ] connection detected need for sudo [ceph-server-3][DEBUG ] connected to host: ceph-server-3 [ceph-server-3][DEBUG ] detect platform information from remote host [ceph-server-3][DEBUG ] detect machine type [ceph-server-3][DEBUG ] find the location of an executable [ceph-server-3][INFO ] Running command: sudo /bin/ip link show [ceph-server-3][INFO ] Running command: sudo /bin/ip addr show [ceph-server-3][DEBUG ] IP addresses found: [u'192.168.0.105'] [ceph_deploy.new][DEBUG ] Resolving host ceph-server-3 [ceph_deploy.new][DEBUG ] Monitor ceph-server-3 at 192.168.0.105 [ceph_deploy.new][DEBUG ] Monitor initial members are ['ceph-server-1', 'ceph-server-2', 'ceph-server-3'] [ceph_deploy.new][DEBUG ] Monitor addrs are ['192.168.0.130', '192.168.0.111', '192.168.0.105'] [ceph_deploy.new][DEBUG ] Creating a random mon key... [ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring... [ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf... admin@ceph-admin:~$ admin@ceph-admin:~$ ssh ceph-client \"sudo ls\" client Desktop Documents Downloads examples.desktop Music Pictures Public Templates Videos xtclient admin@ceph-admin:~$ Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/hardware-recommendations.html":{"url":"docs/hardware-recommendations.html","title":"硬件推荐","keywords":"","body":"本文主要讲 硬件推荐 step Ceph 为普通硬件设计 通常，我们推荐在一台机器上只运行一种类型的守护进程。我们推荐把使用数据集群的进程（如 OpenStack 、 CloudStack 等）安装在别的机器上。 CPU && RAM 部件 OSD Monitor Managers MDSs host(台) >= 3 >= 3 >= 2 cpu(u) >= 2 >=4 ram(GB) bluestor >= (3-5) 与 PG 数相关 >= (1+2) >= (1+2) >= 1 数据存储 硬盘驱动器 单个驱动器容量越大，其对应的 OSD 所需内存就越大，特别是在重均衡、回填、恢复期间。根据经验， 1TB 的存储空间大约需要 1GB 内存。 Ceph 最佳实践指示，你应该分别在单独的硬盘运行操作系统、 OSD 数据和 OSD 日志。 我们推荐独立的驱动器用于安装操作系统和软件，另外每个 OSD 守护进程占用一个驱动器。 允许你在每块硬盘驱动器上运行多个 OSD ，但这会导致资源竞争并降低总体吞吐量。 SSD SSD 用于对象存储太昂贵了，但是把 OSD 的日志存到 SSD 、把对象数据存储到独立的硬盘可以明显提升性能。 控制器 硬盘控制器对写吞吐量也有显著影响，要谨慎地选择，以免产生性能瓶颈。 其他注意事项 如果每台主机运行多个 OSD ，也得保证内核是最新的。 网络 建议每台机器最少两个千兆网卡，分别用于公网（前端）和集群网络（后端）。 故障域 故障域指任何导致不能访问一个或多个 OSD 的故障，可以是主机上停止的进程、硬盘故障、操作系统崩溃、有问题的网卡、损坏的电源、断网、断电等等。规划硬件需求时，要在多个需求间寻求平衡点，像付出很多努力减少故障域带来的成本削减、隔离每个潜在故障域增加的成本。 最低硬件推荐 直接看 这里 对比我们的 env ，应该是 cpu和RAM相对于存储小了。 网卡没达到 千兆网卡。 但是希望不影响我们的研究工作。 ref http://docs.ceph.com/docs/master/start/hardware-recommendations/ Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/master/rados/configuration/common.html":{"url":"docs/master/rados/configuration/common.html","title":"configuration/common","keywords":"","body":"MONITORS 典型的 Ceph 生产集群至少部署 3 个监视器来确保高可靠性。最好奇数个。 在 ceph-server-[1,2,3] 中会有类型下面的文件夹（ceph-admin, ceph-client）没有： vagrant@ceph-server-3:~$ sudo ls /var/lib/ceph/mon/ceph-ceph-server-3/ done keyring kv_backend store.db systemd vagrant@ceph-server-3:~$ OSDS 在 ceph-server-2 和 ceph-server-3 中 (以 ceph-server-2 为例) vagrant@ceph-server-2:~$ sudo ls /var/lib/ceph/osd/ceph-0 -l lrwxrwxrwx 1 root root 15 Feb 23 03:37 /var/lib/ceph/osd/ceph-0 -> /var/local/osd0 vagrant@ceph-server-2:~$ sudo ls /var/lib/ceph/osd/ceph-0 activate.monmap active block bluefs ceph_fsid fsid keyring kv_backend magic mkfs_done ready systemd type whoami vagrant@ceph-server-2:~$ We recommend using the xfs file system when running mkfs. (btrfs and ext4 are not recommended and no longer tested.) See the OSD Config Reference for additional configuration details. 但是，说好得 bluestore 怎么搞？ Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/radosgw/swift/authentication.html":{"url":"docs/radosgw/swift/authentication.html","title":"使用RADOSGW提供ceph的Swift接口","keywords":"","body":"使用RADOSGW提供ceph的Swift接口 env 基于 之前 add-an-rgw-instance-with-quick-install ，因此有： ceph rgw 的 http 接口是： http://ceph-client:8080/ ceph-client ip 是： 192.168.0.134 ，以下只用此IP表示 ceph rgw 接口。 swift client : 192.168.0.51, 此上必须有 swift 命令 step ceph-admin 创建 用户 最好此用户仅供 swift 使用 root@ceph-admin:~# cat ceph-swift-user1-openrc username=swiftuser1 subusername=swiftsubuser1 displayname=swiftuser1displayname password=swiftuser1password root@ceph-admin:~# root@ceph-admin:~# . ceph-swift-user1-openrc root@ceph-admin:~# sudo radosgw-admin user create --subuser=\"${username}:${subusername}\" --uid=\"${username}\" --display-name=\"${displayname}\" --key-type=swift --secret=\"${password}\" --access=full { \"user_id\": \"swiftuser1\", \"display_name\": \"swiftuser1displayname\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [ { \"id\": \"swiftuser1:swiftsubuser1\", \"permissions\": \"full-control\" } ], \"keys\": [], \"swift_keys\": [ { \"user\": \"swiftuser1:swiftsubuser1\", \"secret_key\": \"swiftuser1password\" } ], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"temp_url_keys\": [], \"type\": \"rgw\", \"mfa_ids\": [] } root@ceph-admin:~# swift-client 是否与 ceph rgw 互通 ubuntu@controller:~$ telnet 192.168.0.134 8080 Trying 192.168.0.134... Connected to 192.168.0.134. Escape character is '^]'. ^] telnet> quit ubuntu@controller:~$ 有 swift 命令 ubuntu@controller:~$ swift --version python-swiftclient 3.5.0 ubuntu@controller:~$ 加载变量值 ubuntu@controller:~$ cat ceph-swift-user1-openrc username=swiftuser1 subusername=swiftsubuser1 displayname=swiftuser1displayname password=swiftuser1password ubuntu@controller:~$ . ceph-swift-user1-openrc 查看swift里所有的容器 ubuntu@controller:~$ swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list 创建容器 ubuntu@controller:~$ swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} post swiftuser1-container1 ubuntu@controller:~$ swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 ubuntu@controller:~$ 上传文件 里面没有文件。现在可以上传文件 ubuntu@controller:~$ echo \"Hello World\" > ceph-swiftuser1-container1-object-1.txt ubuntu@controller:~$ swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} upload swiftuser1-container1 ceph-swiftuser1-container1-object-1.txt ceph-swiftuser1-container1-object-1.txt ubuntu@controller:~$ swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 ceph-swiftuser1-container1-object-1.txt 下载文件 ubuntu@controller:~$ mv ceph-swiftuser1-container1-object-1.txt ceph-swiftuser1-container1-object-1.txt.bak ubuntu@controller:~$ ls ceph-swiftuser1-container1-object-1.txt.bak ceph-swift-user1-openrc hello.txt ubuntu@controller:~$ swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} download swiftuser1-container1 ceph-swiftuser1-container1-object-1.txt ceph-swiftuser1-container1-object-1.txt [auth 0.064s, headers 0.087s, total 0.111s, 0.000 MB/s] ubuntu@controller:~$ ls ceph-swiftuser1-container1-object-1.txt ceph-swiftuser1-container1-object-1.txt.bak ceph-swift-user1-openrc hello.txt ubuntu@controller:~$ cat ceph-swiftuser1-container1-object-1.txt Hello World ubuntu@controller:~$ ref http://docs.ceph.com/docs/master/install/install-ceph-gateway/ http://docs.ceph.com/docs/mimic/radosgw/swift/auth/ http://qinghua.github.io/ceph-radosgw/ Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/radosgw/swift/radosgw-swift-get-auth.html":{"url":"docs/radosgw/swift/radosgw-swift-get-auth.html","title":"swift获取auth","keywords":"","body":"radosgw swift auth 之 get auth env 基于 之前 add-an-rgw-instance-with-quick-install ，因此有： ceph rgw 的 http 接口是： http://ceph-client:8080/ ceph-client ip 是： 192.168.0.134 ，以下只用此IP表示 ceph rgw 接口。 swift client : 192.168.0.51, 此上必须有 swift 命令 本文基于 authentication.md 之上。 step ceph-admin 节点 上文设置了 专用于 swift 的帐户密码 现在生成 swift secret key root@ceph-admin:~# cat ceph-swift-user1-openrc username=swiftuser1 subusername=swiftsubuser1 displayname=swiftuser1displayname password=swiftuser1password root@ceph-admin:~# . ceph-swift-user1-openrc root@ceph-admin:~# sudo radosgw-admin key create --subuser=\"${username}:${subusername}\" --key-type=swift --gen-secret { \"user_id\": \"swiftuser1\", \"display_name\": \"swiftuser1displayname\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [ { \"id\": \"swiftuser1:swiftsubuser1\", \"permissions\": \"full-control\" } ], \"keys\": [], \"swift_keys\": [ { \"user\": \"swiftuser1:swiftsubuser1\", \"secret_key\": \"8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k\" } ], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"temp_url_keys\": [], \"type\": \"rgw\", \"mfa_ids\": [] } root@ceph-admin:~# 主要看输出的 swift_keys 部分。 swift client 节点 原来的 password 已失效 注意 因为这里已经生成了 新的secret_key, 则原来的 password 已经失效了。 ubuntu@controller:~$ cat ceph-swift-user1-openrc username=swiftuser1 subusername=swiftsubuser1 displayname=swiftuser1displayname password=swiftuser1password ubuntu@controller:~$ . ceph-swift-user1-openrc ubuntu@controller:~$ swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 Auth GET failed: http://192.168.0.134:8080/auth/v1.0 401 Unauthorized [first 60 chars of response] {\"Code\":\"AccessDenied\",\"RequestId\":\"tx000000000000000000016- Failed Transaction ID: tx000000000000000000016-005c80e8e9-5e2c-default ubuntu@controller:~$ cp ceph-swift-user1-openrc ceph-swift-user1-openrc.password ubuntu@controller:~$ vi ceph-swift-user1-openrc ubuntu@controller:~$ cat ceph-swift-user1-openrc username=swiftuser1 subusername=swiftsubuser1 displayname=swiftuser1displayname password=8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k ubuntu@controller:~$ . ceph-swift-user1-openrc ubuntu@controller:~$ swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 ceph-swiftuser1-container1-object-1.txt dao.txt guo.txt ubuntu@controller:~$ 正确输出X-Auth-Token ubuntu@controller:~$ curl -X GET -H \"X-Auth-User:swiftuser1:swiftsubuser1\" -H \"X-Auth-Key:8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k\" http://192.168.0.134:8080/auth ubuntu@controller:~$ curl -X GET -H \"X-Auth-User:swiftuser1:swiftsubuser1\" -H \"X-Auth-Key:8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k\" http://192.168.0.134:8080/auth -v Note: Unnecessary use of -X or --request, GET is already inferred. * Trying 192.168.0.134... * Connected to 192.168.0.134 (192.168.0.134) port 8080 (#0) > GET /auth HTTP/1.1 > Host: 192.168.0.134:8080 > User-Agent: curl/7.47.0 > Accept: */* > X-Auth-User:swiftuser1:swiftsubuser1 > X-Auth-Key:8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k > （可忽略）参数错误的效果 用户密码均不对 ubuntu@controller:~$ curl -X GET -H \"X-Auth-User:swiftuser1\" -H \"X-Auth-Key:swiftuser1password\" http://192.168.0.134:8080/auth -v Note: Unnecessary use of -X or --request, GET is already inferred. * Trying 192.168.0.134... * Connected to 192.168.0.134 (192.168.0.134) port 8080 (#0) > GET /auth HTTP/1.1 > Host: 192.168.0.134:8080 > User-Agent: curl/7.47.0 > Accept: */* > X-Auth-User:swiftuser1 > X-Auth-Key:swiftuser1password > 用户不对，密码对 ubuntu@controller:~$ curl -X GET -H \"X-Auth-User:swiftuser1\" -H \"X-Auth-Key:8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k\" http://192.168.0.134:8080/auth -v Note: Unnecessary use of -X or --request, GET is already inferred. * Trying 192.168.0.134... * Connected to 192.168.0.134 (192.168.0.134) port 8080 (#0) > GET /auth HTTP/1.1 > Host: 192.168.0.134:8080 > User-Agent: curl/7.47.0 > Accept: */* > X-Auth-User:swiftuser1 > X-Auth-Key:8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k > ref https://blog.csdn.net/u012359453/article/details/79978574 Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/radosgw/swift/radosgw-swift-upload.html":{"url":"docs/radosgw/swift/radosgw-swift-upload.html","title":"swift上传文件","keywords":"","body":"python 通过 python-swiftclient 库实现上传文件至ceph env swift 与 ceph 接口已打通 具体一些环境，可以看下面代码中的参数 ceph rgw: http://192.168.0.134:8080 swift user: 'swiftuser1:swiftsubuser1' swift key : '7Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k' swift container : 'swiftuser1-container1' step 安装包 pip install python-swiftclient 代码 # -*- coding: utf-8 -*- # 测试上传视频2个， 运行格式: `python uploadvideo.py 2` import swiftclient import os import sys import datetime # swift info user = 'swiftuser1:swiftsubuser1' key = '8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k' rootdir = \"./upload/\" count = sys.argv[1] # container name container_name = 'swiftuser1-container1' # connect swift conn = swiftclient.Connection( user=user, key=key, authurl='http://192.168.0.134:8080/auth', ) # uploadfile def uploadFile(pathName,num): if not num.isdigit(): print('please input number') return num = int(num) print('start uploading') start = datetime.datetime.now() # read all file f = open(rootdir + '/video.txt', 'r') file_names = f.readlines() # Guarantee not to cross the borde temp = len(file_names) if num > len(file_names) else num if temp != num: print('The test may be unreasonable Insufficient number of documents') num = temp for i in range(num): filepath = pathName + '/' + file_names[i].strip('\\n') conn.put_object(container_name, file_names[i].strip('\\n'), open(filepath, 'rb')) end = datetime.datetime.now() print('test upload location ' + pathName + ' number: ' + num.__str__() + ' spend time:' + (end-start).total_seconds().__str__() + 'microseconds') # upload file 500 uploadFile(rootdir,num=count) 准备上传文件 这里准备几个小文件 $ ls upload dao.txt dong.txt guo.txt video.txt zuo.txt 运行效果 (cephswift) ➜ CephSwiftTest git:(master) ✗ python uploadvideo.py 2 start uploading test upload location /mnt/f/tmp/upload/ number: 2 spend time:1.272459microseconds (cephswift) ➜ CephSwiftTest git:(master) ✗ 去 swift client 检查 ubuntu@controller:~$ . ceph-swift-user1-openrc ubuntu@controller:~$ swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 ceph-swiftuser1-container1-object-1.txt dao.txt guo.txt ubuntu@controller:~$ 文件在。耶耶耶。成功。 ref https://github.com/JakeRed/CephSwiftTest Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/radosgw/swift/openstack-swift-with-ceph-backend-radosgw.html":{"url":"docs/radosgw/swift/openstack-swift-with-ceph-backend-radosgw.html","title":"ceph集成openstack swift","keywords":"","body":"在 openstack 中用ceph作为 对象存储的后端, python-swiftclient作为客户端，集成openstack keystone. 正在尝试使用radosgw与Ceph（Minic）一起使用Openstack（liberty）swift。目的是将对象存储在ceph osds下。我有一个工作的Openstack和Ceph集群。 要使用Ceph作为对象存储后端，我在ceph集群中安装并配置了radosgw。在openstack节点中，我安装了“python-swiftclient”，创建了一个对象存储服务，并为该服务添加了一个URL为radosgw的端点。 我按照以下链接中的说明进行操作。 http://docs.ceph.com/docs/jewel/radosgw/keystone/ 根据 http://docs.ceph.com/docs/master/radosgw/keystone/ 集成 keystone env 基于 之前 add-an-rgw-instance-with-quick-install ，因此有： ceph rgw 的 http 接口是： http://ceph-client:8080/ ceph-client ip 是： 192.168.0.134 ，以下只用此IP表示 ceph rgw 接口。 ceph-admin: 192.168.0.185 keystone: 192.168.0.51 swift client : 192.168.0.51, 此上必须有 swift 命令 step 确保在集成之前，用户信息有效 Connecting to 192.168.0.51 root@controller:/home/ubuntu# cat ceph-swift-user1-openrc username=swiftuser1 subusername=swiftsubuser1 displayname=swiftuser1displayname password=8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k root@controller:/home/ubuntu# root@controller:/home/ubuntu# . ceph-swift-user1-openrc root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 ceph-swiftuser1-container1-object-1.txt dao.txt guo.txt root@controller:/home/ubuntu# 把 swift 相关配置加入 openstack 查看 openstack endpoint root@controller:~# ls account.ring.gz admin-openrc backups cirros-0.4.0-x86_64-disk.img container.ring.gz demo-openrc object.ring.gz swift-conf tmp tmp2 tmp3 root@controller:~# cat admin-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:~# . admin-openrc root@controller:~# openstack endpoint list +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ | 014c4ed5c42042c394d62a1194bf07ce | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | 057cd51c8ace4daa8834d25ae15998f4 | RegionOne | swift | object-store | True | admin | http://controller:8080/v1 | | 0b3dc5788cac4f2cb66edc3efacf10c0 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | 19e727c1668c4e8aae4315e13057fbbd | RegionOne | cinderv2 | volumev2 | True | public | http://controller:8776/v2/%(project_id)s | | 2548787f06524287b6a27d0a562da375 | RegionOne | glance | image | True | internal | http://controller:9292 | | 295cfee33ff04ab7890a84b47e95bc3f | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | | 50d3b61b60654a65bda26584b4fe0896 | RegionOne | neutron | network | True | admin | http://controller:9696 | | 51952812b675436899ba118585b2dae2 | RegionOne | swift | object-store | True | public | http://swiftproxy:8080/v1/AUTH_%(project_id)s | | 55367bdc8db6438da3a4ed82b1a1b04c | RegionOne | glance | image | True | public | http://controller:9292 | | 5b60dcd8e374410f83e675201d76f06f | RegionOne | placement | placement | True | admin | http://controller:8778 | | 76d61f360fc140a58b4258c344ea9ae5 | RegionOne | nova | compute | True | internal | http://controller:8774/v2.1 | | 7eece3971f9f4105b031214666940d54 | RegionOne | placement | placement | True | public | http://controller:8778 | | 8237211a85314e0c914ea34851e324b7 | RegionOne | cinderv2 | volumev2 | True | admin | http://controller:8776/v2/%(project_id)s | | 87de9f6afbd74792bc80c7d092eb7da6 | RegionOne | swift | object-store | True | internal | http://controller:8080/v1/AUTH_%(project_id)s | | 99b6673fda7f48bb8b6ae11a87ba0e1b | RegionOne | glance | image | True | admin | http://controller:9292 | | a61cbe1448284dd482725fb3067fa25c | RegionOne | nova | compute | True | public | http://controller:8774/v2.1 | | beccdad7270643cabf8a271258c102c0 | RegionOne | swift | object-store | True | admin | http://swiftproxy:8080/v1 | | cfb1df03d1bd42719dd506e0fbee7e5a | RegionOne | cinderv3 | volumev3 | True | internal | http://controller:8776/v3/%(project_id)s | | cfe6b374e86048c98951d079a3cdaa42 | RegionOne | placement | placement | True | internal | http://controller:8778 | | d3f5808482cb42f386c3741b8e1c1d82 | RegionOne | swift | object-store | True | public | http://controller:8080/v1/AUTH_%(project_id)s | | d9bb0785c23143ed855d0bb74dfe53bb | RegionOne | cinderv3 | volumev3 | True | public | http://controller:8776/v3/%(project_id)s | | e1f98d0e79f549a5bc46bcf05705c4b1 | RegionOne | neutron | network | True | public | http://controller:9696 | | e577371ea7e349ffa2a86f7aef2ef35e | RegionOne | swift | object-store | True | internal | http://swiftproxy:8080/v1/AUTH_%(project_id)s | | f089ebe1cb5040319e942dbc607e9930 | RegionOne | cinderv3 | volumev3 | True | admin | http://controller:8776/v3/%(project_id)s | | f3350a1d66fa4813a091beef3782b6fd | RegionOne | neutron | network | True | internal | http://controller:9696 | | f5460d92ef8b428b9dc354628acdb289 | RegionOne | cinderv2 | volumev2 | True | internal | http://controller:8776/v2/%(project_id)s | | fb5c30679ed14e078646041e75e77294 | RegionOne | nova | compute | True | admin | http://controller:8774/v2.1 | +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ root@controller:~# openstack endpoint list | grep object-store | 057cd51c8ace4daa8834d25ae15998f4 | RegionOne | swift | object-store | True | admin | http://controller:8080/v1 | | 51952812b675436899ba118585b2dae2 | RegionOne | swift | object-store | True | public | http://swiftproxy:8080/v1/AUTH_%(project_id)s | | 87de9f6afbd74792bc80c7d092eb7da6 | RegionOne | swift | object-store | True | internal | http://controller:8080/v1/AUTH_%(project_id)s | | beccdad7270643cabf8a271258c102c0 | RegionOne | swift | object-store | True | admin | http://swiftproxy:8080/v1 | | d3f5808482cb42f386c3741b8e1c1d82 | RegionOne | swift | object-store | True | public | http://controller:8080/v1/AUTH_%(project_id)s | | e577371ea7e349ffa2a86f7aef2ef35e | RegionOne | swift | object-store | True | internal | http://swiftproxy:8080/v1/AUTH_%(project_id)s | root@controller:~# openstack endpoint show object-store More than one endpoint exists with the name 'object-store'. root@controller:~# 已经有了 object-store , 删除相应的 endpoint root@controller:~# openstack endpoint delete 057cd51c8ace4daa8834d25ae15998f4 root@controller:~# openstack endpoint delete 51952812b675436899ba118585b2dae2 root@controller:~# openstack endpoint delete 87de9f6afbd74792bc80c7d092eb7da6 root@controller:~# openstack endpoint delete beccdad7270643cabf8a271258c102c0 root@controller:~# openstack endpoint delete d3f5808482cb42f386c3741b8e1c1d82 root@controller:~# openstack endpoint delete e577371ea7e349ffa2a86f7aef2ef35e 添加 openstack service root@controller:~# openstack service create --name=swift \\ > --description=\"Swift Service\" \\ > object-store +-------------|----------------------------------+ | Field | Value | +-------------|----------------------------------+ | description | Swift Service | | enabled | True | | id | c428acff9bb746e8947e99f24e8c5dc8 | | name | swift | | type | object-store | +-------------|----------------------------------+ (可跳过)添加 openstack endpoint 出错 root@controller:~# vi /etc/hosts root@controller:~# openstack endpoint create --region RegionOne \\ > --publicurl \"http://ceph-rgw-client:8080/swift/v1\" \\ > --adminurl \"http://ceph-rgw-client:8080/swift/v1\" \\ > --internalurl \"http://ceph-rgw-client:8080/swift/v1\" \\ > swift usage: openstack endpoint create [-h] [-f {json,shell,table,value,yaml}] [-c COLUMN] [--max-width ] [--fit-width] [--print-empty] [--noindent] [--prefix PREFIX] [--region ] [--enable | --disable] openstack endpoint create: error: argument : invalid choice: u'http://ceph-rgw-client:8080/swift/v1' (choose from 'admin', 'public', 'internal') root@controller:~# 参数不对，那可能使用方式变了。参考 https://docs.openstack.org/swift/rocky/install/controller-install-ubuntu.html root@controller:~# openstack endpoint create --region RegionOne \\ > object-store public http://ceph-rgw-client:8080/swift/v1 Multiple service matches found for 'object-store', use an ID to be more specific. root@controller:~# 加不进，有多个 service 。那应该就是刚刚删除的endpoint的service没删除。 为了不要搞错了，这里再新建立一次。 root@controller:~# openstack service create --name=swift --description=\"Swift Service\" object-store +-------------|----------------------------------+ | Field | Value | +-------------|----------------------------------+ | description | Swift Service | | enabled | True | | id | 356603934f624475b863560b95394ad3 | | name | swift | | type | object-store | +-------------|----------------------------------+ root@controller:~# 查看 service 并删除 root@controller:~# openstack service | grep swift root@controller:~# openstack service list | grep swift | 356603934f624475b863560b95394ad3 | swift | object-store | | a1f85844507d4135b9e77a4c36503552 | swift | object-store | | c428acff9bb746e8947e99f24e8c5dc8 | swift | object-store | root@controller:~# openstack service delete a1f85844507d4135b9e77a4c36503552 root@controller:~# openstack service delete c428acff9bb746e8947e99f24e8c5dc8 root@controller:~# openstack service list | grep swift | 356603934f624475b863560b95394ad3 | swift | object-store | 添加 openstack endpoint root@controller:~# ping ceph-rgw-client root@controller:~# openstack endpoint create --region RegionOne object-store public http://ceph-rgw-client:8080/swift/v1 +--------------|--------------------------------------+ | Field | Value | +--------------|--------------------------------------+ | enabled | True | | id | d2cb83a1aab9467fa9d6d2ede12f9eb7 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 356603934f624475b863560b95394ad3 | | service_name | swift | | service_type | object-store | | url | http://ceph-rgw-client:8080/swift/v1 | +--------------|--------------------------------------+ root@controller:~# openstack endpoint create --region RegionOne \\ > object-store internal http://ceph-rgw-client:8080/swift/v1 +--------------|--------------------------------------+ | Field | Value | +--------------|--------------------------------------+ | enabled | True | | id | 47fa3d7baab34bf4b333b4fbaf269f08 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 356603934f624475b863560b95394ad3 | | service_name | swift | | service_type | object-store | | url | http://ceph-rgw-client:8080/swift/v1 | +--------------|--------------------------------------+ root@controller:~# openstack endpoint create --region RegionOne \\ > object-store admin http://ceph-rgw-client:8080/swift/v1 +--------------|--------------------------------------+ | Field | Value | +--------------|--------------------------------------+ | enabled | True | | id | 96b6fe4b7e404fa4868d06c3d402ba8f | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 356603934f624475b863560b95394ad3 | | service_name | swift | | service_type | object-store | | url | http://ceph-rgw-client:8080/swift/v1 | +--------------|--------------------------------------+ root@controller:~# openstack endpoint list +----------------------------------|-----------|--------------|--------------|---------|-----------|------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------|-----------|--------------|--------------|---------|-----------|------------------------------------------+ | 014c4ed5c42042c394d62a1194bf07ce | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | 0b3dc5788cac4f2cb66edc3efacf10c0 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | 19e727c1668c4e8aae4315e13057fbbd | RegionOne | cinderv2 | volumev2 | True | public | http://controller:8776/v2/%(project_id)s | | 2548787f06524287b6a27d0a562da375 | RegionOne | glance | image | True | internal | http://controller:9292 | | 295cfee33ff04ab7890a84b47e95bc3f | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | | 47fa3d7baab34bf4b333b4fbaf269f08 | RegionOne | swift | object-store | True | internal | http://ceph-rgw-client:8080/swift/v1 | | 50d3b61b60654a65bda26584b4fe0896 | RegionOne | neutron | network | True | admin | http://controller:9696 | | 55367bdc8db6438da3a4ed82b1a1b04c | RegionOne | glance | image | True | public | http://controller:9292 | | 5b60dcd8e374410f83e675201d76f06f | RegionOne | placement | placement | True | admin | http://controller:8778 | | 76d61f360fc140a58b4258c344ea9ae5 | RegionOne | nova | compute | True | internal | http://controller:8774/v2.1 | | 7eece3971f9f4105b031214666940d54 | RegionOne | placement | placement | True | public | http://controller:8778 | | 8237211a85314e0c914ea34851e324b7 | RegionOne | cinderv2 | volumev2 | True | admin | http://controller:8776/v2/%(project_id)s | | 96b6fe4b7e404fa4868d06c3d402ba8f | RegionOne | swift | object-store | True | admin | http://ceph-rgw-client:8080/swift/v1 | | 99b6673fda7f48bb8b6ae11a87ba0e1b | RegionOne | glance | image | True | admin | http://controller:9292 | | a61cbe1448284dd482725fb3067fa25c | RegionOne | nova | compute | True | public | http://controller:8774/v2.1 | | cfb1df03d1bd42719dd506e0fbee7e5a | RegionOne | cinderv3 | volumev3 | True | internal | http://controller:8776/v3/%(project_id)s | | cfe6b374e86048c98951d079a3cdaa42 | RegionOne | placement | placement | True | internal | http://controller:8778 | | d2cb83a1aab9467fa9d6d2ede12f9eb7 | RegionOne | swift | object-store | True | public | http://ceph-rgw-client:8080/swift/v1 | | d9bb0785c23143ed855d0bb74dfe53bb | RegionOne | cinderv3 | volumev3 | True | public | http://controller:8776/v3/%(project_id)s | | e1f98d0e79f549a5bc46bcf05705c4b1 | RegionOne | neutron | network | True | public | http://controller:9696 | | f089ebe1cb5040319e942dbc607e9930 | RegionOne | cinderv3 | volumev3 | True | admin | http://controller:8776/v3/%(project_id)s | | f3350a1d66fa4813a091beef3782b6fd | RegionOne | neutron | network | True | internal | http://controller:9696 | | f5460d92ef8b428b9dc354628acdb289 | RegionOne | cinderv2 | volumev2 | True | internal | http://controller:8776/v2/%(project_id)s | | fb5c30679ed14e078646041e75e77294 | RegionOne | nova | compute | True | admin | http://controller:8774/v2.1 | +----------------------------------|-----------|--------------|--------------|---------|-----------|------------------------------------------+ root@controller:~# openstack endpoint show object-store More than one endpoint exists with the name 'object-store'. root@controller:~# openstack endpoint list |grep -i object | 47fa3d7baab34bf4b333b4fbaf269f08 | RegionOne | swift | object-store | True | internal | http://ceph-rgw-client:8080/swift/v1 | | 96b6fe4b7e404fa4868d06c3d402ba8f | RegionOne | swift | object-store | True | admin | http://ceph-rgw-client:8080/swift/v1 | | d2cb83a1aab9467fa9d6d2ede12f9eb7 | RegionOne | swift | object-store | True | public | http://ceph-rgw-client:8080/swift/v1 | root@controller:~# 好了。这里已经加好了！~ 修改 ceph.conf 配置 因为这里我们不确定写的配置能一次性成功，所以，先在 ceph rgw client 节点直接修改 /etc/ceph/ceph.conf 文件，并重启。 可参考这里 (可跳过)修改配置 root@ceph-client:/home/admin# mkdir /var/ceph/nss -p root@ceph-client:/home/admin# vi /etc/ceph/ceph.conf root@ceph-client:/home/admin# cat /etc/ceph/ceph.conf [global] fsid = 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 mon_initial_members = ceph-server-1, ceph-server-2, ceph-server-3 mon_host = 192.168.0.130,192.168.0.111,192.168.0.105 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx osd pool default size = 2 mon_clock_drift_allowed = 1 [client.rgw.ceph-client] rgw_frontends = \"civetweb port=8080\" rgw_keystone_api_version = v3 rgw_keystone_url = http://controller:5000 rgw_keystone_admin_token = openstack #rgw_keystone_admin_token_path = {path to keystone admin token} #preferred rgw_keystone_accepted_roles = _member_,admin rgw_keystone_token_cache_size = 200 rgw_keystone_revocation_interval = 60 rgw_keystone_admin_domain = default rgw_keystone_admin_project = default rgw_keystone_implicit_tenants = true rgw_keystone_verify_ssl = false nss_db_path = /var/ceph/nss root@ceph-client:/home/admin# (可跳过)重启 rgw 服务 root@ceph-client:/home/admin# systemctl restart ceph-radosgw@rgw.ceph-client root@ceph-client:/home/admin# systemctl status ceph-radosgw@rgw.ceph-client ● ceph-radosgw@rgw.ceph-client.service - Ceph rados gateway Loaded: loaded (/lib/systemd/system/ceph-radosgw@.service; enabled; vendor preset: enabled) Active: active (running) since 一 2019-03-11 18:02:41 CST; 4ms ago Main PID: 83757 ((radosgw)) CGroup: /system.slice/system-ceph\\x2dradosgw.slice/ceph-radosgw@rgw.ceph-client.service └─83757 (radosgw) 3月 11 18:02:41 ceph-client systemd[1]: Started Ceph rados gateway. root@ceph-client:/home/admin# systemctl status ceph-radosgw@rgw.ceph-client ● ceph-radosgw@rgw.ceph-client.service - Ceph rados gateway Loaded: loaded (/lib/systemd/system/ceph-radosgw@.service; enabled; vendor preset: enabled) Active: failed (Result: start-limit-hit) since 一 2019-03-11 18:46:10 CST; 2s ago Process: 84958 ExecStart=/usr/bin/radosgw -f --cluster ${CLUSTER} --name client.%i --setuser ceph --setgroup ceph (code=dumped, signal=ABRT) Main PID: 84958 (code=dumped, signal=ABRT) 3月 11 18:46:09 ceph-client systemd[1]: ceph-radosgw@rgw.ceph-client.service: Main process exited, code=dumped, status=6/ABRT 3月 11 18:46:09 ceph-client systemd[1]: ceph-radosgw@rgw.ceph-client.service: Unit entered failed state. 3月 11 18:46:09 ceph-client systemd[1]: ceph-radosgw@rgw.ceph-client.service: Failed with result 'core-dump'. 3月 11 18:46:10 ceph-client systemd[1]: ceph-radosgw@rgw.ceph-client.service: Service hold-off time over, scheduling restart. 3月 11 18:46:10 ceph-client systemd[1]: Stopped Ceph rados gateway. 3月 11 18:46:10 ceph-client systemd[1]: ceph-radosgw@rgw.ceph-client.service: Start request repeated too quickly. 3月 11 18:46:10 ceph-client systemd[1]: Failed to start Ceph rados gateway. 3月 11 18:46:10 ceph-client systemd[1]: ceph-radosgw@rgw.ceph-client.service: Unit entered failed state. 3月 11 18:46:10 ceph-client systemd[1]: ceph-radosgw@rgw.ceph-client.service: Failed with result 'start-limit-hit'. root@ceph-client:/home/admin# 报错了，啥问题 (可跳过)swift client 验证 root@controller:/home/ubuntu# ls ceph-swiftuser1-container1-object-1.txt ceph-swiftuser1-container1-object-1.txt.bak ceph-swift-user1-openrc ceph-swift-user1-openrc.password hello.txt root@controller:/home/ubuntu# . ceph-swift-user1-openrc root@controller:/home/ubuntu# swift list -v Account GET failed: http://ceph-rgw-client:8080/swift/v1?format=json 401 Unauthorized [first 60 chars of response] {\"Code\":\"AccessDenied\",\"RequestId\":\"tx00000000000000000001f- Failed Transaction ID: tx00000000000000000001f-005c862b5a-5e2c-default root@controller:/home/ubuntu# cat ceph-swift-user1-openrc username=swiftuser1 subusername=swiftsubuser1 displayname=swiftuser1displayname password=8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k root@controller:/home/ubuntu# . ceph-swift-user1-openrc root@controller:/home/ubuntu# swift list -v Account GET failed: http://ceph-rgw-client:8080/swift/v1?format=json 401 Unauthorized [first 60 chars of response] {\"Code\":\"AccessDenied\",\"RequestId\":\"tx000000000000000000021- Failed Transaction ID: tx000000000000000000021-005c862e4a-5e2c-default root@controller:/home/ubuntu# ls ceph-swiftuser1-container1-object-1.txt ceph-swiftuser1-container1-object-1.txt.bak ceph-swift-user1-openrc ceph-swift-user1-openrc.password hello.txt root@controller:/home/ubuntu# . ceph-swift-user1-openrc root@controller:/home/ubuntu# . ceph-swift-user1-openrc root@controller:/home/ubuntu# swift list -v HTTPConnectionPool(host='ceph-rgw-client', port=8080): Max retries exceeded with url: /swift/v1?format=json (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 111] Connection refused',)) root@controller:/home/ubuntu# telnet 192.168.0.134 8080 Trying 192.168.0.134... telnet: Unable to connect to remote host: Connection refused root@controller:/home/ubuntu# 重新配置 还是参考官网 http://docs.ceph.com/docs/master/radosgw/keystone/ ceph rgw 节点直接修改 root@ceph-client:/etc/ceph# ls ceph.client.admin.keyring ceph.client.rgw.ceph-client.keyring ceph.conf rbdmap tmpaKLOQJ tmpqMjag3 root@ceph-client:/etc/ceph# cat ceph.conf [global] fsid = 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 mon_initial_members = ceph-server-1, ceph-server-2, ceph-server-3 mon_host = 192.168.0.130,192.168.0.111,192.168.0.105 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx osd pool default size = 2 mon_clock_drift_allowed = 1 [client.rgw.ceph-client] rgw_frontends = \"civetweb port=8080\" rgw keystone api version = 3 rgw keystone url = http://192.168.0.51:5000 rgw keystone admin user = admin rgw keystone admin password = openstack rgw keystone admin domain = default rgw keystone admin project = admin rgw keystone accepted roles = admin, user rgw keystone token cache size = 500 rgw s3 auth use keystone = true rgw keystone revocation interval = 60 rgw keystone implicit tenants = true root@ceph-client:/etc/ceph# systemctl restart ceph-radosgw@rgw.ceph-client root@ceph-client:/etc/ceph# systemctl status ceph-radosgw@rgw.ceph-client swift client 验证 因为这里只设置了 admin 的role 所以，现在只有admin用户可以通过这个认证 swift client 节点 root@controller:~# . admin-openrc root@controller:~# swift list root@controller:~# . demo-openrc root@controller:~# swift list Account GET failed: http://ceph-rgw-client:8080/swift/v1?format=json 401 Unauthorized [first 60 chars of response] {\"Code\":\"AccessDenied\",\"RequestId\":\"tx000000000000000000003- Failed Transaction ID: tx000000000000000000003-005c877aed-5ebf-default root@controller:~# 看到了吧，由于 demo 用户的 role是 user 而不是 admin, 所以，这里会认证失败的。 怎么让 demo 也可以通过 keystone 认证，当然是在 rgw keystone accepted roles 加上 user 变成下面的样子 rgw keystone accepted roles = admin, user 然后重启rgw, 效果如下。 root@controller:~# . demo-openrc root@controller:~# swift list root@controller:~# 当然，我们也看到了 admin和demo用户 现在是没有container的，是一个全空的用户。 horizon 上传文件 通过chrome访问 https://192.168.0.51/horizon/ 上传一些文件 root@controller:~# . demo-openrc root@controller:~# swift list root@controller:~# swift list helloworld root@controller:~# swift list helloworld DingTalk_v4.6.8.281.exe root@controller:~# ref http://docs.ceph.com/docs/master/radosgw/keystone/ http://docs.ceph.com/docs/jewel/radosgw/keystone/ https://stackoverflow.com/questions/39461803/openstack-swift-with-ceph-backend-radosgw https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html-single/using_keystone_to_authenticate_ceph_object_gateway_users/index Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/radosgw/swift/ceph-radosgw-with-openstack-keystone-v3.html":{"url":"docs/radosgw/swift/ceph-radosgw-with-openstack-keystone-v3.html","title":"ceph与openstack-keystone-v3和swift集成","keywords":"","body":"在 openstack 中用ceph作为 对象存储的后端, python-swiftclient作为客户端，集成openstack keystone v3. 正在尝试使用radosgw与Ceph（Minic）一起使用Openstack（liberty）swift。目的是将对象存储在ceph osds下。我有一个工作的Openstack和Ceph集群。 要使用Ceph作为对象存储后端，我在ceph集群中安装并配置了radosgw。在openstack节点中，我安装了“python-swiftclient”，创建了一个对象存储服务，并为该服务添加了一个URL为radosgw的端点。 根据 http://docs.ceph.com/docs/master/radosgw/keystone/ 集成 keystone env 基于 之前 add-an-rgw-instance-with-quick-install ，因此有： ceph rgw 的 http 接口是： http://ceph-client:8080/ ceph-client ip 是： 192.168.0.134 ，以下只用此IP表示 ceph rgw 接口。 ceph-admin: 192.168.0.185 keystone: 192.168.0.51 swift client : 192.168.0.51, 此上必须有 swift 命令 step 确保在集成之前，用户信息有效 Connecting to 192.168.0.51 root@controller:/home/ubuntu# cat ceph-swift-user1-openrc username=swiftuser1 subusername=swiftsubuser1 displayname=swiftuser1displayname password=8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k root@controller:/home/ubuntu# root@controller:/home/ubuntu# . ceph-swift-user1-openrc root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 ceph-swiftuser1-container1-object-1.txt dao.txt guo.txt root@controller:/home/ubuntu# 把 swift 相关配置加入 openstack 查看 openstack endpoint root@controller:~# ls account.ring.gz admin-openrc backups cirros-0.4.0-x86_64-disk.img container.ring.gz demo-openrc object.ring.gz swift-conf tmp tmp2 tmp3 root@controller:~# cat admin-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:~# . admin-openrc root@controller:~# openstack endpoint list +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ | 014c4ed5c42042c394d62a1194bf07ce | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | 057cd51c8ace4daa8834d25ae15998f4 | RegionOne | swift | object-store | True | admin | http://controller:8080/v1 | | 0b3dc5788cac4f2cb66edc3efacf10c0 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | 19e727c1668c4e8aae4315e13057fbbd | RegionOne | cinderv2 | volumev2 | True | public | http://controller:8776/v2/%(project_id)s | | 2548787f06524287b6a27d0a562da375 | RegionOne | glance | image | True | internal | http://controller:9292 | | 295cfee33ff04ab7890a84b47e95bc3f | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | | 50d3b61b60654a65bda26584b4fe0896 | RegionOne | neutron | network | True | admin | http://controller:9696 | | 51952812b675436899ba118585b2dae2 | RegionOne | swift | object-store | True | public | http://swiftproxy:8080/v1/AUTH_%(project_id)s | | 55367bdc8db6438da3a4ed82b1a1b04c | RegionOne | glance | image | True | public | http://controller:9292 | | 5b60dcd8e374410f83e675201d76f06f | RegionOne | placement | placement | True | admin | http://controller:8778 | | 76d61f360fc140a58b4258c344ea9ae5 | RegionOne | nova | compute | True | internal | http://controller:8774/v2.1 | | 7eece3971f9f4105b031214666940d54 | RegionOne | placement | placement | True | public | http://controller:8778 | | 8237211a85314e0c914ea34851e324b7 | RegionOne | cinderv2 | volumev2 | True | admin | http://controller:8776/v2/%(project_id)s | | 87de9f6afbd74792bc80c7d092eb7da6 | RegionOne | swift | object-store | True | internal | http://controller:8080/v1/AUTH_%(project_id)s | | 99b6673fda7f48bb8b6ae11a87ba0e1b | RegionOne | glance | image | True | admin | http://controller:9292 | | a61cbe1448284dd482725fb3067fa25c | RegionOne | nova | compute | True | public | http://controller:8774/v2.1 | | beccdad7270643cabf8a271258c102c0 | RegionOne | swift | object-store | True | admin | http://swiftproxy:8080/v1 | | cfb1df03d1bd42719dd506e0fbee7e5a | RegionOne | cinderv3 | volumev3 | True | internal | http://controller:8776/v3/%(project_id)s | | cfe6b374e86048c98951d079a3cdaa42 | RegionOne | placement | placement | True | internal | http://controller:8778 | | d3f5808482cb42f386c3741b8e1c1d82 | RegionOne | swift | object-store | True | public | http://controller:8080/v1/AUTH_%(project_id)s | | d9bb0785c23143ed855d0bb74dfe53bb | RegionOne | cinderv3 | volumev3 | True | public | http://controller:8776/v3/%(project_id)s | | e1f98d0e79f549a5bc46bcf05705c4b1 | RegionOne | neutron | network | True | public | http://controller:9696 | | e577371ea7e349ffa2a86f7aef2ef35e | RegionOne | swift | object-store | True | internal | http://swiftproxy:8080/v1/AUTH_%(project_id)s | | f089ebe1cb5040319e942dbc607e9930 | RegionOne | cinderv3 | volumev3 | True | admin | http://controller:8776/v3/%(project_id)s | | f3350a1d66fa4813a091beef3782b6fd | RegionOne | neutron | network | True | internal | http://controller:9696 | | f5460d92ef8b428b9dc354628acdb289 | RegionOne | cinderv2 | volumev2 | True | internal | http://controller:8776/v2/%(project_id)s | | fb5c30679ed14e078646041e75e77294 | RegionOne | nova | compute | True | admin | http://controller:8774/v2.1 | +----------------------------------|-----------|--------------|--------------|---------|-----------|-----------------------------------------------+ root@controller:~# openstack endpoint list | grep object-store | 057cd51c8ace4daa8834d25ae15998f4 | RegionOne | swift | object-store | True | admin | http://controller:8080/v1 | | 51952812b675436899ba118585b2dae2 | RegionOne | swift | object-store | True | public | http://swiftproxy:8080/v1/AUTH_%(project_id)s | | 87de9f6afbd74792bc80c7d092eb7da6 | RegionOne | swift | object-store | True | internal | http://controller:8080/v1/AUTH_%(project_id)s | | beccdad7270643cabf8a271258c102c0 | RegionOne | swift | object-store | True | admin | http://swiftproxy:8080/v1 | | d3f5808482cb42f386c3741b8e1c1d82 | RegionOne | swift | object-store | True | public | http://controller:8080/v1/AUTH_%(project_id)s | | e577371ea7e349ffa2a86f7aef2ef35e | RegionOne | swift | object-store | True | internal | http://swiftproxy:8080/v1/AUTH_%(project_id)s | root@controller:~# openstack endpoint show object-store More than one endpoint exists with the name 'object-store'. root@controller:~# 已经有了 object-store , 删除相应的 endpoint root@controller:~# openstack endpoint delete 057cd51c8ace4daa8834d25ae15998f4 root@controller:~# openstack endpoint delete 51952812b675436899ba118585b2dae2 root@controller:~# openstack endpoint delete 87de9f6afbd74792bc80c7d092eb7da6 root@controller:~# openstack endpoint delete beccdad7270643cabf8a271258c102c0 root@controller:~# openstack endpoint delete d3f5808482cb42f386c3741b8e1c1d82 root@controller:~# openstack endpoint delete e577371ea7e349ffa2a86f7aef2ef35e 同样地， openstack service delete **** 删除相应的 name 是 swift ， type 是 object-store 的 service 添加 openstack service root@controller:~# openstack service create --name=swift \\ > --description=\"Swift Service\" \\ > object-store +-------------|----------------------------------+ | Field | Value | +-------------|----------------------------------+ | description | Swift Service | | enabled | True | | id | c428acff9bb746e8947e99f24e8c5dc8 | | name | swift | | type | object-store | +-------------|----------------------------------+ 添加 openstack endpoint root@controller:~# ping ceph-rgw-client root@controller:~# openstack endpoint create --region RegionOne object-store public http://ceph-rgw-client:8080/swift/v1 +--------------|--------------------------------------+ | Field | Value | +--------------|--------------------------------------+ | enabled | True | | id | d2cb83a1aab9467fa9d6d2ede12f9eb7 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 356603934f624475b863560b95394ad3 | | service_name | swift | | service_type | object-store | | url | http://ceph-rgw-client:8080/swift/v1 | +--------------|--------------------------------------+ root@controller:~# openstack endpoint create --region RegionOne \\ > object-store internal http://ceph-rgw-client:8080/swift/v1 +--------------|--------------------------------------+ | Field | Value | +--------------|--------------------------------------+ | enabled | True | | id | 47fa3d7baab34bf4b333b4fbaf269f08 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 356603934f624475b863560b95394ad3 | | service_name | swift | | service_type | object-store | | url | http://ceph-rgw-client:8080/swift/v1 | +--------------|--------------------------------------+ root@controller:~# openstack endpoint create --region RegionOne \\ > object-store admin http://ceph-rgw-client:8080/swift/v1 +--------------|--------------------------------------+ | Field | Value | +--------------|--------------------------------------+ | enabled | True | | id | 96b6fe4b7e404fa4868d06c3d402ba8f | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 356603934f624475b863560b95394ad3 | | service_name | swift | | service_type | object-store | | url | http://ceph-rgw-client:8080/swift/v1 | +--------------|--------------------------------------+ root@controller:~# openstack endpoint list +----------------------------------|-----------|--------------|--------------|---------|-----------|------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------|-----------|--------------|--------------|---------|-----------|------------------------------------------+ | 014c4ed5c42042c394d62a1194bf07ce | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | 0b3dc5788cac4f2cb66edc3efacf10c0 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | 19e727c1668c4e8aae4315e13057fbbd | RegionOne | cinderv2 | volumev2 | True | public | http://controller:8776/v2/%(project_id)s | | 2548787f06524287b6a27d0a562da375 | RegionOne | glance | image | True | internal | http://controller:9292 | | 295cfee33ff04ab7890a84b47e95bc3f | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | | 47fa3d7baab34bf4b333b4fbaf269f08 | RegionOne | swift | object-store | True | internal | http://ceph-rgw-client:8080/swift/v1 | | 50d3b61b60654a65bda26584b4fe0896 | RegionOne | neutron | network | True | admin | http://controller:9696 | | 55367bdc8db6438da3a4ed82b1a1b04c | RegionOne | glance | image | True | public | http://controller:9292 | | 5b60dcd8e374410f83e675201d76f06f | RegionOne | placement | placement | True | admin | http://controller:8778 | | 76d61f360fc140a58b4258c344ea9ae5 | RegionOne | nova | compute | True | internal | http://controller:8774/v2.1 | | 7eece3971f9f4105b031214666940d54 | RegionOne | placement | placement | True | public | http://controller:8778 | | 8237211a85314e0c914ea34851e324b7 | RegionOne | cinderv2 | volumev2 | True | admin | http://controller:8776/v2/%(project_id)s | | 96b6fe4b7e404fa4868d06c3d402ba8f | RegionOne | swift | object-store | True | admin | http://ceph-rgw-client:8080/swift/v1 | | 99b6673fda7f48bb8b6ae11a87ba0e1b | RegionOne | glance | image | True | admin | http://controller:9292 | | a61cbe1448284dd482725fb3067fa25c | RegionOne | nova | compute | True | public | http://controller:8774/v2.1 | | cfb1df03d1bd42719dd506e0fbee7e5a | RegionOne | cinderv3 | volumev3 | True | internal | http://controller:8776/v3/%(project_id)s | | cfe6b374e86048c98951d079a3cdaa42 | RegionOne | placement | placement | True | internal | http://controller:8778 | | d2cb83a1aab9467fa9d6d2ede12f9eb7 | RegionOne | swift | object-store | True | public | http://ceph-rgw-client:8080/swift/v1 | | d9bb0785c23143ed855d0bb74dfe53bb | RegionOne | cinderv3 | volumev3 | True | public | http://controller:8776/v3/%(project_id)s | | e1f98d0e79f549a5bc46bcf05705c4b1 | RegionOne | neutron | network | True | public | http://controller:9696 | | f089ebe1cb5040319e942dbc607e9930 | RegionOne | cinderv3 | volumev3 | True | admin | http://controller:8776/v3/%(project_id)s | | f3350a1d66fa4813a091beef3782b6fd | RegionOne | neutron | network | True | internal | http://controller:9696 | | f5460d92ef8b428b9dc354628acdb289 | RegionOne | cinderv2 | volumev2 | True | internal | http://controller:8776/v2/%(project_id)s | | fb5c30679ed14e078646041e75e77294 | RegionOne | nova | compute | True | admin | http://controller:8774/v2.1 | +----------------------------------|-----------|--------------|--------------|---------|-----------|------------------------------------------+ root@controller:~# openstack endpoint show object-store More than one endpoint exists with the name 'object-store'. root@controller:~# openstack endpoint list |grep -i object | 47fa3d7baab34bf4b333b4fbaf269f08 | RegionOne | swift | object-store | True | internal | http://ceph-rgw-client:8080/swift/v1 | | 96b6fe4b7e404fa4868d06c3d402ba8f | RegionOne | swift | object-store | True | admin | http://ceph-rgw-client:8080/swift/v1 | | d2cb83a1aab9467fa9d6d2ede12f9eb7 | RegionOne | swift | object-store | True | public | http://ceph-rgw-client:8080/swift/v1 | root@controller:~# 好了。这里已经加好了！~ 修改 ceph.conf 配置 因为这里我们不确定写的配置能一次性成功，所以，先在 ceph rgw client 节点直接修改 /etc/ceph/ceph.conf 文件，并重启。 重新配置 还是参考官网 http://docs.ceph.com/docs/master/radosgw/keystone/ ceph rgw 节点直接修改 root@ceph-client:/etc/ceph# ls ceph.client.admin.keyring ceph.client.rgw.ceph-client.keyring ceph.conf rbdmap tmpaKLOQJ tmpqMjag3 root@ceph-client:/etc/ceph# cat ceph.conf [global] fsid = 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 mon_initial_members = ceph-server-1, ceph-server-2, ceph-server-3 mon_host = 192.168.0.130,192.168.0.111,192.168.0.105 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx osd pool default size = 2 mon_clock_drift_allowed = 1 [client.rgw.ceph-client] rgw_frontends = \"civetweb port=8080\" rgw keystone api version = 3 rgw keystone url = http://192.168.0.51:5000 rgw keystone admin user = admin rgw keystone admin password = openstack rgw keystone admin domain = default rgw keystone admin project = admin rgw keystone accepted roles = admin, user rgw keystone token cache size = 500 rgw s3 auth use keystone = true rgw keystone revocation interval = 60 rgw keystone implicit tenants = true root@ceph-client:/etc/ceph# systemctl restart ceph-radosgw@rgw.ceph-client root@ceph-client:/etc/ceph# systemctl status ceph-radosgw@rgw.ceph-client swift client 验证 因为这里只设置了 admin 的role 所以，现在只有admin用户可以通过这个认证 swift client 节点 root@controller:~# . admin-openrc root@controller:~# swift list root@controller:~# . demo-openrc root@controller:~# swift list Account GET failed: http://ceph-rgw-client:8080/swift/v1?format=json 401 Unauthorized [first 60 chars of response] {\"Code\":\"AccessDenied\",\"RequestId\":\"tx000000000000000000003- Failed Transaction ID: tx000000000000000000003-005c877aed-5ebf-default root@controller:~# 看到了吧，由于 demo 用户的 role是 user 而不是 admin, 所以，这里会认证失败的。 怎么让 demo 也可以通过 keystone 认证，当然是在 rgw keystone accepted roles 加上 user 变成下面的样子 rgw keystone accepted roles = admin, user 然后重启rgw, 效果如下。 root@controller:~# . demo-openrc root@controller:~# swift list root@controller:~# 当然，我们也看到了 admin和demo用户 现在是没有container的，是一个全空的用户。 horizon 上传文件 通过chrome访问 https://192.168.0.51/horizon/ 上传一些文件 root@controller:~# . demo-openrc root@controller:~# swift list root@controller:~# swift list helloworld root@controller:~# swift list helloworld DingTalk_v4.6.8.281.exe root@controller:~# ref http://docs.ceph.com/docs/master/radosgw/keystone/ http://docs.ceph.com/docs/jewel/radosgw/keystone/ https://stackoverflow.com/questions/39461803/openstack-swift-with-ceph-backend-radosgw https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html-single/using_keystone_to_authenticate_ceph_object_gateway_users/index Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/radosgw/swift/radosgw-swift-setup-tmp-url.html":{"url":"docs/radosgw/swift/radosgw-swift-setup-tmp-url.html","title":"设置Temporary URL","keywords":"","body":"配置 openrc 方式 root@controller:/home/ubuntu# cat admin-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:/home/ubuntu# . admin-openrc root@controller:/home/ubuntu# 参数 方式 root@controller:/home/ubuntu# swift --os-auth-url http://controller:5000/v3 --auth-version 3 --os-project-name admin --os-project-domain-name Default --os-username admin --os-user-domain-name Default --os-password openstack list abcd App.ico hello.txt world.txt root@controller:/home/ubuntu# 设置并生成tmp url 命令行方式 root@controller:/home/ubuntu# swift post -m \"Temp-URL-Key:admintmpurlkey\" py方式 tmp_url.py 内容如下 import requests import json print(\"设置Temp URL------------------------------------------------\") token = \"AUTH_rgwtk18000000737769667475736572313a737769667473756275736572313568fcfafc4ff2bf40e3cd567292dd2786f6771b677628913b927020413a5debf2db55b4\" URL = 'http://192.168.0.134:8080/swift/v1/' headers = {'X-Auth-Token':token, 'X-Account-Meta-Temp-URL-Key':\"hello\"} res = requests.post(URL,headers=headers) print(res.text) print(res.headers) print(res.status_code) print(\"查看有没有设置成功------------------------------------------------\") URL = 'http://192.168.0.134:8080/swift/v1/' # headers = {'X-Auth-Token':token, 'X-Account-Meta-Temp-URL-Key':\"hello\"} headers = {'X-Auth-Token':token} res = requests.get(URL,headers=headers) print(res.text) print(res.headers) print(res.status_code) print(\"生成 tmp url------------------------------------------------\") import hmac from hashlib import sha1 from time import time method = 'GET' host = 'http://192.168.0.134:8080/swift' duration_in_seconds = 300 # Duration for which the url is valid expires = int(time() + duration_in_seconds) path = '/v1/swiftuser1-container1/ceph-swiftuser1-container1-object-1.txt' key = 'hello' hmac_body = '%s\\n%s\\n%s' % (method, expires, path) key = bytes(key , 'utf-8') hmac_body = bytes(hmac_body, 'utf-8') sig = hmac.new(key, hmac_body, sha1).hexdigest() rest_uri = \"{host}{path}?temp_url_sig={sig}&temp_url_expires={expires}\".format( host=host, path=path, sig=sig, expires=expires) print(rest_uri) 验证 命令行方式 swift stat -v 输出的 Meta Temp-Url-Key 就是。 root@controller:/home/ubuntu# swift stat -v StorageURL: http://ceph-rgw-client:8080/swift/v1 Auth Token: gAAAAABcrBT-mEGVWB-gx706oHzO87Fl8c9XR5F8DmhC_rvSaefWISL9Y_hmYw_Ubf5HlY0K_UISlp14U2ZubTayDhTMtqS7UF4r2xxWP9BMczLCOWJHeu80Z00HVVH-GSWfbyO8n-dNpCiCNmEPF8aYzt6M9ZcWwv_D4a9M4pWfxu4GwGQk9qw Account: v1 Containers: 1 Objects: 3 Bytes: 25529 Objects in policy \"default-placement-bytes\": 0 Bytes in policy \"default-placement-bytes\": 0 Containers in policy \"default-placement\": 1 Objects in policy \"default-placement\": 3 Bytes in policy \"default-placement\": 25529 Meta Temp-Url-Key: admintmpurlkey Accept-Ranges: bytes X-Timestamp: 1554781438.70248 X-Account-Bytes-Used-Actual: 36864 X-Trans-Id: tx000000000000000000463-005cac14fe-392d5-default Content-Type: text/plain; charset=utf-8 X-Openstack-Request-Id: tx000000000000000000463-005cac14fe-392d5-default root@controller:/home/ubuntu# py方式 URL = 'http://192.168.0.134:8080/swift/v1/' # headers = {'X-Auth-Token':token, 'X-Account-Meta-Temp-URL-Key':\"hello\"} headers = {'X-Auth-Token':token} res = requests.get(URL,headers=headers) print(res.text) print(res.headers) print(res.status_code) 运行与输出 中的 'X-Account-Meta-Temp-Url-Key': 'admintmpurlkey' 就是。 (swift) ➜ utils git:(master) ✗ python u.py abcd {'X-Timestamp': '1554781444.03614', 'X-Account-Container-Count': '1', 'X-Account-Object-Count': '3', 'X-Account-Bytes-Used': '25529', 'X-Account-Bytes-Used-Actual': '36864', 'X-Account-Storage-Policy-Default-Placement-Container-Count': '1', 'X-Account-Storage-Policy-Default-Placement-Object-Count': '3', 'X-Account-Storage-Policy-Default-Placement-Bytes-Used': '25529', 'X-Account-Storage-Policy-Default-Placement-Bytes-Used-Actual': '36864', 'X-Account-Meta-Temp-Url-Key': 'admintmpurlkey', 'Accept-Ranges': 'bytes', 'X-Trans-Id': 'tx000000000000000000464-005cac1503-392d5-default', 'X-Openstack-Request-Id': 'tx000000000000000000464-005cac1503-392d5-default', 'Content-Type': 'text/plain; charset=utf-8', 'Content-Length': '4', 'Date': 'Tue, 09 Apr 2019 03:44:04 GMT', 'Connection': 'Keep-Alive'} 200 (swift) ➜ utils git:(master) ✗ ref https://docs.openstack.org/kilo/config-reference/content/object-storage-tempurl.html Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/radosgw/swift/radosgw-swift-tmp-url-no-keystone.html":{"url":"docs/radosgw/swift/radosgw-swift-tmp-url-no-keystone.html","title":"(非keystone认证)swift生成并获取tmp url","keywords":"","body":"当用户不通过keystone认证时，设置并生成tmp url env 基于 之前 add-an-rgw-instance-with-quick-install ，因此有： ceph rgw 的 http 接口是： http://ceph-client:8080/ ceph-client ip 是： 192.168.0.134 ，以下只用此IP表示 ceph rgw 接口。 ceph用户一个。注意：此用户是直接通过，radosgw-admin user create 创建，并非 keystone 用户。 endpoint 中 service 是： rgw 节点配置文件/etc/ceph.conf 并没有配置： swift account in url = true step 查看帐户，容器，对象信息 root@controller:/home/ubuntu# cat ceph-swift-user1-openrc username=swiftuser1 subusername=swiftsubuser1 displayname=swiftuser1displayname password=G8jTHyahPudXOQ0Dv3oDn3wq9fMKnbcc9e4IumZO root@controller:/home/ubuntu# . ceph-swift-user1-openrc 要获取上面的password等字段，则通过： radosgw-admin user info --uid=\"swiftuser1\" 得到 sudo radosgw-admin key create --subuser=\"${username}:${subusername}\" --key-type=swift --gen-secret 生成并替换 root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 ceph-swiftuser1-container1-object-1.txt dao.txt guo.txt root@controller:/home/ubuntu# 这里有一个 ceph-swiftuser1-container1-object-1.txt 对象，等会我们就把这个对象生成tmp url 来访问。 得到token root@controller:~# curl -X GET -H \"X-Auth-User:swiftuser1:swiftsubuser1\" -H \"X-Auth-Key:G8jTHyahPudXOQ0Dv3oDn3wq9fMKnbcc9e4IumZO\" http://192.168.0.134:8080/auth -v Note: Unnecessary use of -X or --request, GET is already inferred. * Trying 192.168.0.134... * Connected to 192.168.0.134 (192.168.0.134) port 8080 (#0) > GET /auth HTTP/1.1 > Host: 192.168.0.134:8080 > User-Agent: curl/7.47.0 > Accept: */* > X-Auth-User:swiftuser1:swiftsubuser1 > X-Auth-Key:G8jTHyahPudXOQ0Dv3oDn3wq9fMKnbcc9e4IumZO > 好了，我们要的token 就是： AUTH_rgwtk18000000737769667475736572313a737769667473756275736572313568fcfafc4ff2bf40e3cd567292dd2786f6771b677628913b927020413a5debf2db55b4 设置并生成tmp url tmp_url.py 内容如下 import requests import json print(\"设置Temp URL------------------------------------------------\") token = \"AUTH_rgwtk18000000737769667475736572313a737769667473756275736572313568fcfafc4ff2bf40e3cd567292dd2786f6771b677628913b927020413a5debf2db55b4\" URL = 'http://192.168.0.134:8080/swift/v1/' headers = {'X-Auth-Token':token, 'X-Account-Meta-Temp-URL-Key':\"hello\"} res = requests.post(URL,headers=headers) print(res.text) print(res.headers) print(res.status_code) print(\"查看有没有设置成功------------------------------------------------\") URL = 'http://192.168.0.134:8080/swift/v1/' # headers = {'X-Auth-Token':token, 'X-Account-Meta-Temp-URL-Key':\"hello\"} headers = {'X-Auth-Token':token} res = requests.get(URL,headers=headers) print(res.text) print(res.headers) print(res.status_code) print(\"生成 tmp url------------------------------------------------\") import hmac from hashlib import sha1 from time import time method = 'GET' host = 'http://192.168.0.134:8080/swift' duration_in_seconds = 300 # Duration for which the url is valid expires = int(time() + duration_in_seconds) path = '/v1/swiftuser1-container1/ceph-swiftuser1-container1-object-1.txt' key = 'hello' hmac_body = '%s\\n%s\\n%s' % (method, expires, path) key = bytes(key , 'utf-8') hmac_body = bytes(hmac_body, 'utf-8') sig = hmac.new(key, hmac_body, sha1).hexdigest() rest_uri = \"{host}{path}?temp_url_sig={sig}&temp_url_expires={expires}\".format( host=host, path=path, sig=sig, expires=expires) print(rest_uri) （可忽略）生成tmp url部分如下： method = 'GET' host = 'http://192.168.0.134:8080/swift' duration_in_seconds = 300 # Duration for which the url is valid expires = int(time() + duration_in_seconds) path = '/v1/123/abc.txt' key = 'hello' hmac_body = '%s\\n%s\\n%s' % (method, expires, path) key = bytes(key , 'utf-8') hmac_body = bytes(hmac_body, 'utf-8') sig = hmac.new(key, hmac_body, sha1).hexdigest() rest_uri = \"{host}{path}?temp_url_sig={sig}&temp_url_expires={expires}\".format( host=host, path=path, sig=sig, expires=expires) print(rest_uri) python tmp_url.py ------------------------------------------------ Traceback (most recent call last): File \"tmp_url.py\", line 118, in sig = hmac.new(key, hmac_body, sha1).hexdigest() File \"/root/.pyenv/versions/3.7.2/lib/python3.7/hmac.py\", line 153, in new return HMAC(key, msg, digestmod) File \"/root/.pyenv/versions/3.7.2/lib/python3.7/hmac.py\", line 49, in __init__ raise TypeError(\"key: expected bytes or bytearray, but got %r\" % type(key).__name__) TypeError: key: expected bytes or bytearray, but got 'str' 参考 https://stackoverflow.com/questions/31848293/python3-and-hmac-how-to-handle-string-not-being-binary 知道，要将 key, hmac_body转成bytes key = bytes(key , 'utf-8') hmac_body = bytes(hmac_body, 'utf-8') 运行python tmp_url.py后，得到 http://192.168.0.134:8080/swift/v1/swiftuser1-container1/ceph-swiftuser1-container1-object-1.txt?temp_url_sig=f223797922ab7e4371c93d619f4bceac98069a3c&temp_url_expires=1554287683 但是，放到 chrome 中： 直接下载了一个 ceph-swiftuser1-container1-object-1.txt 查看一下 ceph-swiftuser1-container1-object-1.txt 文件内容，没毛病，内容正确。 ref http://docs.ceph.com/docs/master/radosgw/swift/tempurl/ https://blog.csdn.net/u014104588/article/details/86248675?tdsourcetag=s_pcqq_aiomsg http://docs.ceph.org.cn/radosgw/swift/tempurl/ Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/radosgw/swift/radosgw-swift-tmp-url-keystone.html":{"url":"docs/radosgw/swift/radosgw-swift-tmp-url-keystone.html","title":"(keystone认证)swift生成并获取tmp url","keywords":"","body":"当用户通过keystone认证时，设置并生成tmp url 注意：本文没有操作成功。也就是说，本文只是记录，没有给出解决方案。 step tmp_url.py 内容如下 print(\"拿 token ----------------------------------------------\") import requests import json URL = 'http://192.168.0.51:5000/v3/auth/tokens' body = { \"auth\": { \"identity\": { \"methods\": [ \"password\" ], \"password\": { \"user\": { \"name\": \"admin\", \"domain\": { \"name\": \"default\" }, \"password\": \"openstack\" } } }, \"scope\": { \"project\": { \"domain\": { \"name\": \"default\" }, \"name\": \"admin\" } } } } body = json.dumps(body) headers = {'Content-Type':'application/json'} res = requests.post(URL,data=body,headers=headers) token =res.headers['X-Subject-Token'] project_id = json.loads(res.content)[\"token\"][\"project\"][\"id\"] print(res.headers['X-Subject-Token']) print(\"上传一个文件，先准备个文件哟。如果已经有文件，此步可忽略------------------------------------------------\") with open('abc.txt') as f: data = f.read() URL = 'http://192.168.0.134:8080/swift/v1/123/abc.txt' headers = {'X-Auth-Token':token} res = requests.put(URL,headers=headers,data=data) print(res.text) print(res.headers) print(res.status_code) print(\"设置Temp URL------------------------------------------------\") import hmac from hashlib import sha1 from time import time URL = 'http://192.168.0.134:8080/swift/v1/' headers = {'X-Auth-Token':token, 'X-Account-Meta-Temp-URL-Key':\"hello\"} res = requests.post(URL,headers=headers) print(res.text) print(res.headers) print(res.status_code) print(\"查看有没有设置成功------------------------------------------------\") URL = 'http://192.168.0.134:8080/swift/v1/' headers = {'X-Auth-Token':token, 'X-Account-Meta-Temp-URL-Key':\"hello\"} res = requests.get(URL,headers=headers) print(res.text) print(res.headers) print(res.status_code) print(\"生成 tmp url------------------------------------------------\") import hmac from hashlib import sha1 from time import time method = 'GET' host = 'http://192.168.0.134:8080/swift' duration_in_seconds = 300 # Duration for which the url is valid expires = int(time() + duration_in_seconds) path = '/v1/123/abc.txt' key = 'hello' hmac_body = '%s\\n%s\\n%s' % (method, expires, path) key = bytes(key , 'utf-8') hmac_body = bytes(hmac_body, 'utf-8') sig = hmac.new(key, hmac_body, sha1).hexdigest() rest_uri = \"{host}{path}?temp_url_sig={sig}&temp_url_expires={expires}\".format( host=host, path=path, sig=sig, expires=expires) print(rest_uri) 生成tmp url部分如下： method = 'GET' host = 'http://192.168.0.134:8080/swift' duration_in_seconds = 300 # Duration for which the url is valid expires = int(time() + duration_in_seconds) path = '/v1/123/abc.txt' key = 'hello' hmac_body = '%s\\n%s\\n%s' % (method, expires, path) key = bytes(key , 'utf-8') hmac_body = bytes(hmac_body, 'utf-8') sig = hmac.new(key, hmac_body, sha1).hexdigest() rest_uri = \"{host}{path}?temp_url_sig={sig}&temp_url_expires={expires}\".format( host=host, path=path, sig=sig, expires=expires) print(rest_uri) python tmp_url.py ------------------------------------------------ Traceback (most recent call last): File \"tmp_url.py\", line 118, in sig = hmac.new(key, hmac_body, sha1).hexdigest() File \"/root/.pyenv/versions/3.7.2/lib/python3.7/hmac.py\", line 153, in new return HMAC(key, msg, digestmod) File \"/root/.pyenv/versions/3.7.2/lib/python3.7/hmac.py\", line 49, in __init__ raise TypeError(\"key: expected bytes or bytearray, but got %r\" % type(key).__name__) TypeError: key: expected bytes or bytearray, but got 'str' 参考 https://stackoverflow.com/questions/31848293/python3-and-hmac-how-to-handle-string-not-being-binary 知道，要将 key, hmac_body转成bytes key = bytes(key , 'utf-8') hmac_body = bytes(hmac_body, 'utf-8') 运行后，得到 http://192.168.0.134:8080/swift/v1/123/abc.txt?temp_url_sig=8acedc3134554c69de5d3fb42cbd9981dc9a16e4&temp_url_expires=1554286239 但是，放到 chrome 中： 显示 AccessDenied 什么情况？ update your Keystone endpoint 查一下 root@controller:/home/ubuntu# . admin-openrc root@controller:/home/ubuntu# openstack endpoint list +----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------+ | 1c1c7306ea024eb98fae578ed1383f99 | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | 4fa93f815d64428697bcc41e56f38d94 | RegionOne | swift | object-store | True | internal | http://ceph-rgw-client:8080/swift/v1 | | 5be5403aecb9418aa73288b0f56e1315 | RegionOne | swift | object-store | True | public | http://ceph-rgw-client:8080/swift/v1 | | b7df0ccdceba42ce8272d26882627e33 | RegionOne | swift | object-store | True | admin | http://ceph-rgw-client:8080/swift/v1 | | b9499be1f9ac4ad8ad9705f2e1a13847 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | badb42c672534b8290a37b828b758c9d | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | +----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------+ root@controller:/home/ubuntu# 方向 根据 http://docs.ceph.com/docs/master/radosgw/swift/tempurl/ 的提示的 update your Keystone endpoint to the URL suffix /v1/AUTH_%(tenant_id)s (instead of just /v1). 操作 我们知道，在 keystone v3 中已经没有 tenant_id 了，替换成 project_id 。 也就是说，我们要把 swift 的 endpoint 由： http://ceph-rgw-client:8080/swift/v1 替换成 http://ceph-rgw-client:8080/swift/v1/AUTH_%(project_id)s 更新endpoint有2种方式： 数据库方式 database: keystone table: endpoint 命令行方式修改参考 https://docs.openstack.org/python-openstackclient/pike/cli/command-objects/endpoint.html 因为我们不知道keystone对应的mysql安装在哪里（也不需要知道），所以，我们选择命令行方式 操作 root@controller:/home/ubuntu# openstack endpoint set --url \"http://ceph-rgw-client:8080/swift/v1/AUTH_%(project_id)s\" 4fa93f815d64428697bcc41e56f38d94 root@controller:/home/ubuntu# openstack endpoint set --url \"http://ceph-rgw-client:8080/swift/v1/AUTH_%(project_id)s\" 5be5403aecb9418aa73288b0f56e1315 root@controller:/home/ubuntu# openstack endpoint set --url \"http://ceph-rgw-client:8080/swift/v1/AUTH_%(project_id)s\" b7df0ccdceba42ce8272d26882627e33 root@controller:/home/ubuntu# openstack endpoint list +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------------------------------------+ | 1c1c7306ea024eb98fae578ed1383f99 | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | 4fa93f815d64428697bcc41e56f38d94 | RegionOne | swift | object-store | True | internal | http://ceph-rgw-client:8080/swift/v1/AUTH_%(project_id)s | | 5be5403aecb9418aa73288b0f56e1315 | RegionOne | swift | object-store | True | public | http://ceph-rgw-client:8080/swift/v1/AUTH_%(project_id)s | | b7df0ccdceba42ce8272d26882627e33 | RegionOne | swift | object-store | True | admin | http://ceph-rgw-client:8080/swift/v1/AUTH_%(project_id)s | | b9499be1f9ac4ad8ad9705f2e1a13847 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | badb42c672534b8290a37b828b758c9d | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------------------------------------+ root@controller:/home/ubuntu# url 要加 双引号，否则报错如下： root@controller:/home/ubuntu# openstack endpoint set --url http://ceph-rgw-client:8080/swift/v1/AUTH_%(project_id)s 4fa93f815d64428697bcc41e56f38d94 bash: syntax error near unexpected token `(' root@controller:/home/ubuntu# 查询，这时，会报错 root@controller:/home/ubuntu# . admin-openrc root@controller:/home/ubuntu# swift list Account not found root@controller:/home/ubuntu# 回头再看文档，发现，是不是要同时修改 rgw节点的 /etc/ceph.conf 文件。 实际证明，是的。两个地方要同时修改。两处是相互配合的。如果只修改其中一处，则都会报错。 具体如下： /etc/ceph.conf root@ceph-client:/etc/ceph# cat ceph.conf [global] fsid = 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 mon_initial_members = ceph-server-1, ceph-server-2, ceph-server-3 mon_host = 192.168.0.130,192.168.0.111,192.168.0.105 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx osd pool default size = 2 mon_clock_drift_allowed = 1 rgw swift account in url = true [client.rgw.ceph-client] rgw_frontends = \"civetweb port=8080\" rgw keystone api version = 3 rgw keystone url = http://192.168.0.51:5000 rgw keystone admin user = admin rgw keystone admin password = openstack rgw keystone admin domain = default rgw keystone admin project = admin rgw keystone accepted roles = admin, user rgw keystone token cache size = 500 rgw s3 auth use keystone = true rgw keystone revocation interval = 60 rgw keystone implicit tenants = true root@ceph-client:/etc/ceph# root@ceph-client:/etc/ceph# sudo systemctl restart ceph-radosgw@rgw.ceph-client.service openstack endpoint list root@controller:/home/ubuntu# . admin-openrc root@controller:/home/ubuntu# openstack endpoint list | grep object-store | 4fa93f815d64428697bcc41e56f38d94 | RegionOne | swift | object-store | True | internal | http://ceph-rgw-client:8080/swift/v1/AUTH_%(project_id)s | | 5be5403aecb9418aa73288b0f56e1315 | RegionOne | swift | object-store | True | public | http://ceph-rgw-client:8080/swift/v1/AUTH_%(project_id)s | | b7df0ccdceba42ce8272d26882627e33 | RegionOne | swift | object-store | True | admin | http://ceph-rgw-client:8080/swift/v1/AUTH_%(project_id)s | root@controller:/home/ubuntu# 这样就意味着： 上传下载URL 由 http://192.168.0.134:8080/swift/v1/ 更新为： http://192.168.0.134:8080/swift/v1/AUTH_%(project_id)s/ ( 就是多加 AUTH_(project_id)s/ ) 因为修改了 ceph.conf ，那把配置分发一下。 ceph-admin节点： admin@ceph-admin:~/test-cluster$ ceph-deploy --overwrite-conf config push ceph-admin ceph-client ceph-server-1 ceph-server-2 ceph-server-3 然后，再验证一下。（因为在ceph-admin节点无法运行ssh ceph-server-1 命令，发现 ceph-server-1 节点有问题，要修复这个ssh问题，所以暂停了。） 换思路，对比日志情况 tail -f /var/log/ceph/ceph-client.rgw.ceph-client.log 原生用户 tmp url 日志 2019-04-09 10:46:18.303 7fa3d7a12700 1 ====== starting new request req=0x7fa3d7a09830 ===== 2019-04-09 10:46:18.987 7fa3d7a12700 1 ====== req done req=0x7fa3d7a09830 op status=1902 http_status=204 ====== 2019-04-09 10:46:18.987 7fa3d7a12700 1 civetweb: 0x205c000: 192.168.0.158 - - [09/Apr/2019:10:46:18 +0800] \"POST /swift/v1/ HTTP/1.1\" 204 265 - python-requests/2.21.0 2019-04-09 10:46:18.995 7fa3d7a12700 1 ====== starting new request req=0x7fa3d7a09830 ===== 2019-04-09 10:46:19.003 7fa3d7a12700 1 ====== req done req=0x7fa3d7a09830 op status=0 http_status=200 ====== 2019-04-09 10:46:19.003 7fa3d7a12700 1 civetweb: 0x205c000: 192.168.0.158 - - [09/Apr/2019:10:46:18 +0800] \"GET /swift/v1/ HTTP/1.1\" 200 755 - python-requests/2.21.0 chrome： http://192.168.0.134:8080/swift/v1/swiftuser1-container1/ceph-swiftuser1-container1-object-1.txt?temp_url_sig=b9a878f2293eb67ddf2f68ee044fd1445791d596&temp_url_expires=1554778278 2019-04-09 10:46:34.251 7fa3d7211700 1 ====== starting new request req=0x7fa3d7208830 ===== 2019-04-09 10:46:34.259 7fa3d7211700 1 ====== req done req=0x7fa3d7208830 op status=0 http_status=200 ====== 2019-04-09 10:46:34.259 7fa3d7211700 1 civetweb: 0x205c9d8: 192.168.0.158 - - [09/Apr/2019:10:46:34 +0800] \"GET /swift/v1/swiftuser1-container1/ceph-swiftuser1-container1-object-1.txt?temp_url_sig=b9a878f2293eb67ddf2f68ee044fd1445791d596&temp_url_expires=1554778278 HTTP/1.1\" 200 538 - Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36 2019-04-09 10:46:42.351 7fa3d8a14700 0 ERROR: keystone revocation processing returned error r=-5 keystone 用户 日志 2019-04-09 10:44:46.925 7fa3d7a12700 1 ====== starting new request req=0x7fa3d7a09830 ===== 2019-04-09 10:44:47.477 7fa3d7a12700 0 validated token: demo:demo expires: 1555987486 2019-04-09 10:44:47.825 7fa3d7a12700 1 ====== req done req=0x7fa3d7a09830 op status=1902 http_status=204 ====== 2019-04-09 10:44:47.825 7fa3d7a12700 1 civetweb: 0x205c000: 192.168.0.158 - - [09/Apr/2019:10:44:46 +0800] \"POST /swift/v1 HTTP/1.1\" 204 265 - python-requests/2.21.0 2019-04-09 10:44:47.837 7fa3d7a12700 1 ====== starting new request req=0x7fa3d7a09830 ===== 2019-04-09 10:44:47.849 7fa3d7a12700 1 ====== req done req=0x7fa3d7a09830 op status=0 http_status=200 ====== 2019-04-09 10:44:47.849 7fa3d7a12700 1 civetweb: 0x205c000: 192.168.0.158 - - [09/Apr/2019:10:44:47 +0800] \"GET /swift/v1 HTTP/1.1\" 200 762 - python-requests/2.21.0 chrome： http://192.168.0.134:8080/swift/v1/abcd/hello.txt?temp_url_sig=d5b943ad55c2aa8ed3d391bd917d99be84f9fa9f&temp_url_expires=1554778187 2019-04-09 10:45:22.138 7fa3d7a12700 1 ====== starting new request req=0x7fa3d7a09830 ===== 2019-04-09 10:45:22.142 7fa3d7a12700 1 ====== req done req=0x7fa3d7a09830 op status=0 http_status=403 ====== 2019-04-09 10:45:22.142 7fa3d7a12700 1 civetweb: 0x205c000: 192.168.0.158 - - [09/Apr/2019:10:45:22 +0800] \"GET /swift/v1/abcd/hello.txt?temp_url_sig=d5b943ad55c2aa8ed3d391bd917d99be84f9fa9f&temp_url_expires=1554778187 HTTP/1.1\" 403 318 - Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36 返回的是 403 资源不可用. 嘛意思？ 查看 rgw用户与 keystone用户的区别 下面我们以 rgw用户 swiftuser1 与 keystone用户 a9e83dfd0fa14ec4b2e47842f2d5797a（demo）为例子，进行展示 a9e83dfd0fa14ec4b2e47842f2d5797a admin@ceph-admin:~$ radosgw-admin user info --uid \"a9e83dfd0fa14ec4b2e47842f2d5797a\\$a9e83dfd0fa14ec4b2e47842f2d5797a\" { \"user_id\": \"a9e83dfd0fa14ec4b2e47842f2d5797a$a9e83dfd0fa14ec4b2e47842f2d5797a\", \"display_name\": \"demo\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [], \"keys\": [], \"swift_keys\": [], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"temp_url_keys\": [ { \"key\": 0, \"val\": \"demohelloworld\" } ], \"type\": \"keystone\", \"mfa_ids\": [] } admin@ceph-admin:~$ swiftuser1 admin@ceph-admin:~$ radosgw-admin user info --uid \"swiftuser1\" { \"user_id\": \"swiftuser1\", \"display_name\": \"swiftuser1displayname\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [ { \"id\": \"swiftuser1:swiftsubuser1\", \"permissions\": \"full-control\" } ], \"keys\": [], \"swift_keys\": [ { \"user\": \"swiftuser1:swiftsubuser1\", \"secret_key\": \"G8jTHyahPudXOQ0Dv3oDn3wq9fMKnbcc9e4IumZO\" } ], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"temp_url_keys\": [ { \"key\": 0, \"val\": \"hello\" } ], \"type\": \"rgw\", \"mfa_ids\": [] } admin@ceph-admin:~$ 通过观察，知道，我们的 keystone 用户，没有 swift_keys 配置。 这个值，可以通过 sudo radosgw-admin key create --subuser=\"${username}:${subusername}\" --key-type=swift --gen-secret 生成并替换 但是，subuser 也没有设定，怎么办？ 配置 subuser 和 swift_keys 我们知道 rgw 用户 是通过下面的方式创建的 root@ceph-admin:~# cat ceph-swift-user1-openrc username=swiftuser1 subusername=swiftsubuser1 displayname=swiftuser1displayname password=swiftuser1password root@ceph-admin:~# root@ceph-admin:~# . ceph-swift-user1-openrc root@ceph-admin:~# sudo radosgw-admin user create --subuser=\"${username}:${subusername}\" --uid=\"${username}\" --display-name=\"${displayname}\" --key-type=swift --secret=\"${password}\" --access=full 相同的，我们试一下 admin@ceph-admin:~/test-data$ cat ceph-keystone-demo username=a9e83dfd0fa14ec4b2e47842f2d5797a\\$a9e83dfd0fa14ec4b2e47842f2d5797a subusername=demo displayname=demo password=openstack admin@ceph-admin:~/test-data$ . ceph-keystone-demo admin@ceph-admin:~/test-data$ sudo radosgw-admin user create --subuser=\"${username}:${subusername}\" --uid=\"${username}\" --display-name=\"${displayname}\" --key-type=swift --secret=\"${password}\" --access=full could not create subuser: unable to create subuser, unable to store user info admin@ceph-admin:~/test-data$ 失败了。 所以，只能尝试 sudo radosgw-admin user modify admin@ceph-admin:~/test-data$ radosgw-admin user modify --uid=\"${username}\" --subuser=\"${username}:${subusername}\" --display-name=\"${displayname}\" --key-type=swift --secret=\"${password}\" --access=full { \"user_id\": \"a9e83dfd0fa14ec4b2e47842f2d5797a$a9e83dfd0fa14ec4b2e47842f2d5797a\", \"display_name\": \"demo\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [], \"keys\": [], \"swift_keys\": [ { \"user\": \"a9e83dfd0fa14ec4b2e47842f2d5797a$a9e83dfd0fa14ec4b2e47842f2d5797a:demo\", \"secret_key\": \"openstack\" } ], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"temp_url_keys\": [ { \"key\": 0, \"val\": \"demohelloworld\" } ], \"type\": \"keystone\", \"mfa_ids\": [] } admin@ceph-admin:~/test-data$ 在 swift-client 节点尝试一下，获取 token root@controller:/home/ubuntu# curl -X GET -H \"X-Auth-User:a9e83dfd0fa14ec4b2e47842f2d5797a\\$a9e83dfd0fa14ec4b2e47842f2d5797a:demo\" -H \"X-Auth-Key:openstack\" http://192.168.0.134:8080/auth -v Note: Unnecessary use of -X or --request, GET is already inferred. * Trying 192.168.0.134... * Connected to 192.168.0.134 (192.168.0.134) port 8080 (#0) > GET /auth HTTP/1.1 > Host: 192.168.0.134:8080 > User-Agent: curl/7.47.0 > Accept: */* > X-Auth-User:a9e83dfd0fa14ec4b2e47842f2d5797a$a9e83dfd0fa14ec4b2e47842f2d5797a:demo > X-Auth-Key:openstack > 拿到了。 利用token进行设置 tmp url 发现会报 403 。 再对比，发现 \"subusers\": [] 。也就是说给这个 subusers 设置了 swift_key, 但是，没有添加 subuser 用户。 admin@ceph-admin:~/test-data$ radosgw-admin subuser create --uid=\"${username}\" --subuser=\"${username}:${subusername}\" --display-name=\"${displayname}\" --key-type=swift --secret=\"${password}\" --access=full { \"user_id\": \"a9e83dfd0fa14ec4b2e47842f2d5797a$a9e83dfd0fa14ec4b2e47842f2d5797a\", \"display_name\": \"demo\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [ { \"id\": \"a9e83dfd0fa14ec4b2e47842f2d5797a$a9e83dfd0fa14ec4b2e47842f2d5797a:demo\", \"permissions\": \"full-control\" } ], \"keys\": [], \"swift_keys\": [ { \"user\": \"a9e83dfd0fa14ec4b2e47842f2d5797a$a9e83dfd0fa14ec4b2e47842f2d5797a:demo\", \"secret_key\": \"openstack\" } ], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"temp_url_keys\": [ { \"key\": 0, \"val\": \"demohelloworld2222222222222222222\" } ], \"type\": \"keystone\", \"mfa_ids\": [] } admin@ceph-admin:~/test-data$ 这下都有了。 生成 token root@controller:/home/ubuntu# curl -X GET -H \"X-Auth-User:a9e83dfd0fa14ec4b2e47842f2d5797a\\$a9e83dfd0fa14ec4b2e47842f2d5797a:demo\" -H \"X-Auth-Key:openstack\" http://192.168.0.134:8080/auth -v Note: Unnecessary use of -X or --request, GET is already inferred. * Trying 192.168.0.134... * Connected to 192.168.0.134 (192.168.0.134) port 8080 (#0) > GET /auth HTTP/1.1 > Host: 192.168.0.134:8080 > User-Agent: curl/7.47.0 > Accept: */* > X-Auth-User:a9e83dfd0fa14ec4b2e47842f2d5797a$a9e83dfd0fa14ec4b2e47842f2d5797a:demo > X-Auth-Key:openstack > 生成 tmp url key import requests import json token = \"AUTH_rgwtk4600000061396538336466643066613134656334623265343738343266326435373937612461396538336466643066613134656334623265343738343266326435373937613a64656d6f61190960e628bb4a7f9aad5ce3f24b168054f7204cba88919a36fd3eac665283c179bbcc\" import hmac from hashlib import sha1 from time import time key = 'demohelloworld444444444444444' URL = 'http://192.168.0.134:8080/swift/v1/' URL = 'http://192.168.0.134:8080/swift/v1' headers = {'X-Auth-Token':token, 'X-Account-Meta-Temp-URL-Key':key} res = requests.post(URL,headers=headers) print(res.text) print(res.headers) print(res.status_code) URL = 'http://192.168.0.134:8080/swift/v1/' URL = 'http://192.168.0.134:8080/swift/v1' headers = {'X-Auth-Token':token, 'X-Account-Meta-Temp-URL-Key':key} res = requests.get(URL,headers=headers) print(res.text) print(res.headers) print(res.status_code) print(\"------------------------------------------------\") 生成 tmp url import hmac from hashlib import sha1 from time import time method = 'GET' host = 'http://192.168.0.134:8080/swift' # host = 'http://192.168.0.51:5000/v3/auth/tokens' duration_in_seconds = 300 # Duration for which the url is valid expires = int(time() + duration_in_seconds) path = '/v1/abcd/hello.txt' hmac_body = '%s\\n%s\\n%s' % (method, expires, path) key = bytes(key , 'utf-8') hmac_body = bytes(hmac_body, 'utf-8') sig = hmac.new(key, hmac_body, sha1).hexdigest() rest_uri = \"{host}{path}?temp_url_sig={sig}&temp_url_expires={expires}\".format( host=host, path=path, sig=sig, expires=expires) print(rest_uri) chrome: http://192.168.0.134:8080/swift/v1/abcd/hello.txt?temp_url_sig=cfca82e1b725919ef684b2d14a36c664cdb20bf4&temp_url_expires=1554795078 失败。 ref http://docs.ceph.com/docs/master/radosgw/swift/tempurl/ https://blog.csdn.net/u014104588/article/details/86248675?tdsourcetag=s_pcqq_aiomsg http://docs.ceph.org.cn/radosgw/swift/tempurl/ Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/radosgw/swift/get-user-info.html":{"url":"docs/radosgw/swift/get-user-info.html","title":"radosgw用户信息","keywords":"","body":"列出所有用户 radosgw-admin user list radosgw-admin metadata list user admin@ceph-admin:~$ radosgw-admin user list [ \"1c0130df9066484791e2807ca65ce8c2$1c0130df9066484791e2807ca65ce8c2\", \"5ed7a713486547ae80f6c05d0da42a54$5ed7a713486547ae80f6c05d0da42a54\", \"03b11e496f45412f909fd5e58221d6c8$03b11e496f45412f909fd5e58221d6c8\", \"6a5bca70568e4b6fb7cb5f1447f160c2$6a5bca70568e4b6fb7cb5f1447f160c2\", \"a05c6b0541c44f49955ed1748b3fcfdb$a05c6b0541c44f49955ed1748b3fcfdb\", \"23c12f3f66fb40ccb565d58cf3402c54$23c12f3f66fb40ccb565d58cf3402c54\", \"41260b903ce44ab7b5c28e13132ce816$41260b903ce44ab7b5c28e13132ce816\", \"d1f1fb325ea84c58b4820dae96dcb721$d1f1fb325ea84c58b4820dae96dcb721\", \"a9e83dfd0fa14ec4b2e47842f2d5797a$a9e83dfd0fa14ec4b2e47842f2d5797a\", \"f04ec0abf3d1460dad82608bb03af589$f04ec0abf3d1460dad82608bb03af589\", \"swiftuser1\", \"4553832d3bf14c9a853b89c2992235b9$4553832d3bf14c9a853b89c2992235b9\", \"7d6eaa90d74a4f239963933c3a744df3$7d6eaa90d74a4f239963933c3a744df3\", \"446002de5b8c41e69685351a92d26a2d$446002de5b8c41e69685351a92d26a2d\" ] admin@ceph-admin:~$ 获取用户信息 root@ceph-client:/etc/ceph# radosgw-admin metadata list user [ \"swiftuser1\" ] root@ceph-client:/etc/ceph# radosgw-admin user info --uid=\"swiftuser1\" { \"user_id\": \"swiftuser1\", \"display_name\": \"swiftuser1displayname\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [ { \"id\": \"swiftuser1:swiftsubuser1\", \"permissions\": \"full-control\" } ], \"keys\": [], \"swift_keys\": [ { \"user\": \"swiftuser1:swiftsubuser1\", \"secret_key\": \"8Us5blLxYeh77cqWQR9qhgWT3e8BMOu8zokeKB3k\" } ], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"temp_url_keys\": [], \"type\": \"rgw\", \"mfa_ids\": [] } root@ceph-client:/etc/ceph# 建个 openrc 文件 root@controller:/home/ubuntu# cat ceph-swift-user1-openrc username=swiftuser1 subusername=swiftsubuser1 displayname=swiftuser1displayname password=G8jTHyahPudXOQ0Dv3oDn3wq9fMKnbcc9e4IumZO root@controller:/home/ubuntu# . ceph-swift-user1-openrc 获取上面的password 要获取上面的password等字段，则通过： radosgw-admin user info --uid=\"swiftuser1\" 生成并替换password sudo radosgw-admin key create --subuser=\"${username}:${subusername}\" --key-type=swift --gen-secret 生成并替换 rgw admin@ceph-admin:~$ radosgw-admin user info --uid \"swiftuser1\" { \"user_id\": \"swiftuser1\", \"display_name\": \"swiftuser1displayname\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [ { \"id\": \"swiftuser1:swiftsubuser1\", \"permissions\": \"full-control\" } ], \"keys\": [], \"swift_keys\": [ { \"user\": \"swiftuser1:swiftsubuser1\", \"secret_key\": \"G8jTHyahPudXOQ0Dv3oDn3wq9fMKnbcc9e4IumZO\" } ], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"temp_url_keys\": [ { \"key\": 0, \"val\": \"hello\" } ], \"type\": \"rgw\", \"mfa_ids\": [] } admin@ceph-admin:~$ keystone keystone 用户，user_id 使用的是openstack project_id 名（不是openstack user_id）, 即：radosgw下的 user_id 为 ${project_id}$${project_id}。 比如下面这个示例：当时是通过keystone生成的用户demo, 其 project_id 是 a9e83dfd0fa14ec4b2e47842f2d5797a ，则 所以下面 \"type\": \"keystone\", \"user_id\": \"a9e83dfd0fa14ec4b2e47842f2d5797a$a9e83dfd0fa14ec4b2e47842f2d5797a\", temp_url_keys 也是明文展示在这里 注意：在写参数的时候，要在 $ 号前加 \\ 转义 admin@ceph-admin:~$ radosgw-admin user info --uid \"a9e83dfd0fa14ec4b2e47842f2d5797a\\$a9e83dfd0fa14ec4b2e47842f2d5797a\" { \"user_id\": \"a9e83dfd0fa14ec4b2e47842f2d5797a$a9e83dfd0fa14ec4b2e47842f2d5797a\", \"display_name\": \"demo\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [], \"keys\": [], \"swift_keys\": [], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"temp_url_keys\": [ { \"key\": 0, \"val\": \"demohelloworld\" } ], \"type\": \"keystone\", \"mfa_ids\": [] } admin@ceph-admin:~$ ref https://keithtenzer.com/2017/03/30/openstack-swift-integration-with-ceph/ Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/faq/ntpdate.html":{"url":"docs/faq/ntpdate.html","title":"时间不一致","keywords":"","body":"env admin@ceph-admin:~$ ceph -s cluster: id: 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 health: HEALTH_WARN application not enabled on 1 pool(s) clock skew detected on mon.ceph-server-2, mon.ceph-server-1 services: mon: 3 daemons, quorum ceph-server-3,ceph-server-2,ceph-server-1 mgr: ceph-server-1(active) osd: 2 osds: 2 up, 2 in rgw: 1 daemon active data: pools: 7 pools, 56 pgs objects: 227 objects, 2.0 KiB usage: 2.0 GiB used, 7.3 TiB / 7.3 TiB avail pgs: 56 active+clean admin@ceph-admin:~$ 根据提示clock skew detected on mon.ceph-server-2, mon.ceph-server-1, 说明是时间不对。观察后，知道，各组件之间的时间不一致。 ntpdate 保证同一时间 安装与校对时间 sudo apt update && sudo apt install ntpdate -y sudo ntpdate cn.pool.ntp.org && date 再观察 admin@ceph-server-1:~$ ceph -s cluster: id: 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 health: HEALTH_WARN application not enabled on 1 pool(s) services: mon: 3 daemons, quorum ceph-server-3,ceph-server-2,ceph-server-1 mgr: ceph-server-1(active) osd: 2 osds: 2 up, 2 in data: pools: 7 pools, 56 pgs objects: 227 objects, 2.0 KiB usage: 2.0 GiB used, 7.3 TiB / 7.3 TiB avail pgs: 56 active+clean admin@ceph-server-1:~$ 时间问题解决。 Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/faq/rgw-conf-change.html":{"url":"docs/faq/rgw-conf-change.html","title":"修改rgw配置生效","keywords":"","body":"修改了 ceph.conf 中的关于 ceph rgw 部分，如何让配置生效 env step 配置文件更新 在 ceph-admin 节点 admin@ceph-admin:~/test-cluster$ ceph-deploy --overwrite-conf config push ceph-admin ceph-client [ceph_deploy.conf][DEBUG ] found configuration file at: /home/admin/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy --overwrite-conf config push ceph-admin ceph-client [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : True [ceph_deploy.cli][INFO ] subcommand : push [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] client : ['ceph-admin', 'ceph-client'] [ceph_deploy.cli][INFO ] func : [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.config][DEBUG ] Pushing config to ceph-admin [ceph-admin][DEBUG ] connection detected need for sudo [ceph-admin][DEBUG ] connected to host: ceph-admin [ceph-admin][DEBUG ] detect platform information from remote host [ceph-admin][DEBUG ] detect machine type [ceph-admin][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf [ceph_deploy.config][DEBUG ] Pushing config to ceph-client [ceph-client][DEBUG ] connection detected need for sudo [ceph-client][DEBUG ] connected to host: ceph-client [ceph-client][DEBUG ] detect platform information from remote host [ceph-client][DEBUG ] detect machine type [ceph-client][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf admin@ceph-admin:~/test-cluster$ 重启 ceph rgw 节点 在 ceph-client 节点 root@ceph-client:/home/admin# systemctl restart ceph-radosgw@rgw.ceph-client root@ceph-client:/home/admin# systemctl status ceph-radosgw@rgw.ceph-client ● ceph-radosgw@rgw.ceph-client.service - Ceph rados gateway Loaded: loaded (/lib/systemd/system/ceph-radosgw@.service; enabled; vendor preset: enabled) Active: active (running) since 一 2019-03-11 17:55:58 CST; 1s ago Main PID: 82967 (radosgw) CGroup: /system.slice/system-ceph\\x2dradosgw.slice/ceph-radosgw@rgw.ceph-client.service └─82967 /usr/bin/radosgw -f --cluster ceph --name client.rgw.ceph-client --setuser ceph --setgroup ceph 3月 11 17:55:58 ceph-client systemd[1]: Started Ceph rados gateway. root@ceph-client:/home/admin# ref https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html-single/using_keystone_to_authenticate_ceph_object_gateway_users/index Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/faq/get-service-name.html":{"url":"docs/faq/get-service-name.html","title":"查找服务名称","keywords":"","body":"通过 systemctl status 查找服务名称 比如，我现在把 rgw 安装在了一台机器上，但是，我不知道rgw的service全名是什么，这时，我修改了配置文件，就会不知道怎么重启服务。 这里可以按下面方式找到服务。 root@ceph-client:/etc/ceph# systemctl status | grep rados │ └─101359 grep --color=auto rados ├─system-ceph\\x2dradosgw.slice │ └─ceph-radosgw@rgw.ceph-client.service │ └─101280 /usr/bin/radosgw -f --cluster ceph --name client.rgw.ceph-client --setuser ceph --setgroup ceph root@ceph-client:/etc/ceph# systemctl status ceph-radosgw@rgw.ceph-client ● ceph-radosgw@rgw.ceph-client.service - Ceph rados gateway Loaded: loaded (/lib/systemd/system/ceph-radosgw@.service; enabled; vendor preset: enabled) Active: active (running) since 二 2019-03-12 15:44:27 CST; 21s ago Main PID: 101280 (radosgw) CGroup: /system.slice/system-ceph\\x2dradosgw.slice/ceph-radosgw@rgw.ceph-client.service └─101280 /usr/bin/radosgw -f --cluster ceph --name client.rgw.ceph-client --setuser ceph --setgroup ceph 3月 12 15:44:27 ceph-client systemd[1]: Started Ceph rados gateway. root@ceph-client:/etc/ceph# Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/radosgw/basic.html":{"url":"docs/radosgw/basic.html","title":"支持断点续传","keywords":"","body":"支持 断点续传 https://blog.csdn.net/younger_china/article/details/73410727 Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/faq/ceph-deploy-osd-error.html":{"url":"docs/faq/ceph-deploy-osd-error.html","title":"ceph-deploy osd create时报错","keywords":"","body":"ceph-deploy osd create --data /dev/sda1 ceph-server-2 时报错如下： env step 参考 http://docs.ceph.org.cn/rados/deployment/ceph-deploy-osd/ 能找到 磁盘，但是，没有显示 ceph 盘 ref http://docs.ceph.org.cn/rados/deployment/ceph-deploy-osd/ Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/faq/swift-list-error.html":{"url":"docs/faq/swift-list-error.html","title":"swift-list临时出错","keywords":"","body":"env 基于 之前 add-an-rgw-instance-with-quick-install ，因此有： ceph rgw 的 http 接口是： http://ceph-client:8080/ ceph-client ip 是： 192.168.0.134 ，以下只用此IP表示 ceph rgw 接口。 ceph-admin: 192.168.0.185 keystone: 192.168.0.51 swift client : 192.168.0.51, 此上必须有 swift 命令 现象 之前正常的。突然某个时间点 swift list 报错了 root@controller:~# . admin-openrc root@controller:~# swift list Account HEAD failed: http://ceph-rgw-client:8080/swift/v1 401 Unauthorized Failed Transaction ID: tx000000000000000000104-0056bcd49c-5ec1-default root@controller:~# 或者 在 访问 horizon ： http://192.168.0.51/horizon/project/containers/ 时，会自动退出，并跳到登录页面。 step 原理：因为 ceph-rgw-client 重启了一次。 解决：重启ceph rgw 节点 root@ceph-client:/etc/ceph# systemctl restart ceph-radosgw@rgw.ceph-client root@ceph-client:/etc/ceph# systemctl status ceph-radosgw@rgw.ceph-client ● ceph-radosgw@rgw.ceph-client.service - Ceph rados gateway Loaded: loaded (/lib/systemd/system/ceph-radosgw@.service; enabled; vendor preset: enabled) Active: active (running) since 五 2016-02-12 02:42:28 CST; 11s ago Main PID: 4109 (radosgw) CGroup: /system.slice/system-ceph\\x2dradosgw.slice/ceph-radosgw@rgw.ceph-client.service └─4109 /usr/bin/radosgw -f --cluster ceph --name client.rgw.ceph-client --setuser ceph --setgroup ceph 2月 12 02:42:28 ceph-client systemd[1]: Started Ceph rados gateway. root@ceph-client:/etc/ceph# 再次访问就正常了 root@controller:~# . admin-openrc root@controller:~# swift list 123 test_1 root@controller:~# Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/radosgw/swift/radosgw-swift-keystone-mysql-change.html":{"url":"docs/radosgw/swift/radosgw-swift-keystone-mysql-change.html","title":"keystone更换mysql","keywords":"","body":"当keystone 相关联的数据库 出问题时，mysql作迁移后，ceph与swift集成所受影响 结论： 不受影响。 如果未迁移数据内容，则需要添加swift 与 ceph 集成的endpoint。（操作可参考 swift 与 ceph 集成操作） env 因其它原因，keystone关联的mysql出错，无法使用，用数据转至其它机器时 基于 之前 add-an-rgw-instance-with-quick-install ，因此有： ceph rgw 的 http 接口是： http://ceph-client:8080/ ceph-client ip 是： 192.168.0.134 ，以下只用此IP表示 ceph rgw 接口。 ceph用户一个。注意：此用户是直接通过，radosgw-admin user create 创建，并非 keystone 用户。 endpoint 中 service 是： rgw 节点配置文件/etc/ceph.conf 并没有配置： swift account in url = true step 直接获取不到容器信息 root@controller:/home/ubuntu# cat admin-openrc export OS_PROJECT_DOMAIN_NAME=Default export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=openstack export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 root@controller:/home/ubuntu# root@controller:/home/ubuntu# . admin-openrc root@controller:/home/ubuntu# swift list Endpoint for object-store not found - have you specified a region? root@controller:/home/ubuntu# 确保在集成之前，用户信息有效 root@controller:/home/ubuntu# . ceph-swift-user1-openrc root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list Authorization Failure. Authorization failed: Method Not Allowed (HTTP 405) root@controller:/home/ubuntu# root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 Authorization Failure. Authorization failed: Method Not Allowed (HTTP 405) root@controller:/home/ubuntu# 这里报出来的是 405 错误，与之前的 401 不同。 那说明，ceph本身有问题 注意：此时rgw接口依然是有效的，所以仅通过 http://192.168.0.134:8080/ 可访问来判断ceph集群是否正常，是无依据的。 来到 ceph-admin 节点 0.185 admin@ceph-admin:~$ ceph -s cluster: id: 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 health: HEALTH_WARN application not enabled on 1 pool(s) clock skew detected on mon.ceph-server-1 services: mon: 3 daemons, quorum ceph-server-3,ceph-server-2,ceph-server-1 mgr: ceph-server-1(active) osd: 2 osds: 2 up, 2 in rgw: 1 daemon active data: pools: 7 pools, 56 pgs objects: 2.28 k objects, 3.7 GiB usage: 9.5 GiB used, 7.3 TiB / 7.3 TiB avail pgs: 56 active+clean admin@ceph-admin:~$ 这个知道的，时间问题 http://eiuapp.github.io/ceph-handbook/docs/faq/ntpdate.html 解决后 admin@ceph-admin:~$ ceph -s cluster: id: 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 health: HEALTH_WARN application not enabled on 1 pool(s) services: mon: 3 daemons, quorum ceph-server-3,ceph-server-2,ceph-server-1 mgr: ceph-server-1(active) osd: 2 osds: 2 up, 2 in rgw: 1 daemon active data: pools: 7 pools, 56 pgs objects: 2.28 k objects, 3.7 GiB usage: 9.5 GiB used, 7.3 TiB / 7.3 TiB avail pgs: 56 active+clean admin@ceph-admin:~$ 这个是 application not enabled 问题，参考 http://eiuapp.github.io/ceph-handbook/docs/faq/ntpdate.html admin@ceph-admin:~$ ceph -s cluster: id: 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 health: HEALTH_WARN application not enabled on 1 pool(s) services: mon: 3 daemons, quorum ceph-server-3,ceph-server-2,ceph-server-1 mgr: ceph-server-1(active) osd: 2 osds: 2 up, 2 in rgw: 1 daemon active data: pools: 7 pools, 56 pgs objects: 2.28 k objects, 3.7 GiB usage: 9.5 GiB used, 7.3 TiB / 7.3 TiB avail pgs: 56 active+clean admin@ceph-admin:~$ ceph health detail HEALTH_WARN application not enabled on 1 pool(s) POOL_APP_NOT_ENABLED application not enabled on 1 pool(s) application not enabled on pool 'mytest' use 'ceph osd pool application enable ', where is 'cephfs', 'rbd', 'rgw', or freeform for custom applications. admin@ceph-admin:~$ ceph osd pool application enable mytest rbd enabled application 'rbd' on pool 'mytest' admin@ceph-admin:~$ ceph -s cluster: id: 87aa8ebd-2782-415d-bf23-ad2eb89b61c6 health: HEALTH_OK services: mon: 3 daemons, quorum ceph-server-3,ceph-server-2,ceph-server-1 mgr: ceph-server-1(active) osd: 2 osds: 2 up, 2 in rgw: 1 daemon active data: pools: 7 pools, 56 pgs objects: 2.28 k objects, 3.7 GiB usage: 9.5 GiB used, 7.3 TiB / 7.3 TiB avail pgs: 56 active+clean admin@ceph-admin:~$ 好了，现在 ceph 健康了。 注意，这个时候，不应该使用原来的 窗口 查询 swift 信息，应该另开一个窗口来查询。不然，还是会得到错误。具体原因，不知道。 root@controller:/home/ubuntu# . ceph-swift-user1-openrc root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list Authorization Failure. Authorization failed: Method Not Allowed (HTTP 405) root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list Authorization Failure. Authorization failed: Unable to establish connection to http://192.168.0.134:8080/auth/v1.0/auth/tokens root@controller:/home/ubuntu# swift用户信息 root@controller:/home/ubuntu# . admin-openrc root@controller:/home/ubuntu# swift stat -v Authorization Failure. Authorization failed: An unexpected error prevented the server from fulfilling your request. (HTTP 500) (Request-ID: req-71675d8c-3e88-42b7-85ad-b285b26ac1d9) root@controller:/home/ubuntu# 因为之前，我们知道，这个应该是 endpoint list 没有加入swift环境导致。这样我们就要把之前的 swift 与 ceph 集成的操作再做一次。 swift 与 ceph 集成 root@controller:/home/ubuntu# ping ceph-rgw-client PING ceph-rgw-client (192.168.0.134) 56(84) bytes of data. 64 bytes from ceph-rgw-client (192.168.0.134): icmp_seq=1 ttl=64 time=1.66 ms ^C --- ceph-rgw-client ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.666/1.666/1.666/0.000 ms root@controller:/home/ubuntu# openstack endpoint list +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------+ | 1c1c7306ea024eb98fae578ed1383f99 | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | b9499be1f9ac4ad8ad9705f2e1a13847 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | badb42c672534b8290a37b828b758c9d | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | +----------------------------------+-----------+--------------+--------------+---------+-----------+----------------------------+ root@controller:/home/ubuntu# openstack service create --name=swift \\ > --description=\"Swift Service\" \\ > object-store +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Swift Service | | enabled | True | | id | 632c97c278784f5caae69cc9fe9df549 | | name | swift | | type | object-store | +-------------+----------------------------------+ root@controller:/home/ubuntu# openstack endpoint create --region RegionOne object-store public http://ceph-rgw-client:8080/swift/v1 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | 5be5403aecb9418aa73288b0f56e1315 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 632c97c278784f5caae69cc9fe9df549 | | service_name | swift | | service_type | object-store | | url | http://ceph-rgw-client:8080/swift/v1 | +--------------+--------------------------------------+ root@controller:/home/ubuntu# openstack endpoint create --region RegionOne object-store internal http://ceph-rgw-client:8080/swift/v1 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | 4fa93f815d64428697bcc41e56f38d94 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 632c97c278784f5caae69cc9fe9df549 | | service_name | swift | | service_type | object-store | | url | http://ceph-rgw-client:8080/swift/v1 | +--------------+--------------------------------------+ root@controller:/home/ubuntu# openstack endpoint create --region RegionOne object-store admin http://ceph-rgw-client:8080/swift/v1 +--------------+--------------------------------------+ | Field | Value | +--------------+--------------------------------------+ | enabled | True | | id | b7df0ccdceba42ce8272d26882627e33 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 632c97c278784f5caae69cc9fe9df549 | | service_name | swift | | service_type | object-store | | url | http://ceph-rgw-client:8080/swift/v1 | +--------------+--------------------------------------+ root@controller:/home/ubuntu# openstack endpoint list +----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------+ | 1c1c7306ea024eb98fae578ed1383f99 | RegionOne | keystone | identity | True | internal | http://controller:5000/v3/ | | 4fa93f815d64428697bcc41e56f38d94 | RegionOne | swift | object-store | True | internal | http://ceph-rgw-client:8080/swift/v1 | | 5be5403aecb9418aa73288b0f56e1315 | RegionOne | swift | object-store | True | public | http://ceph-rgw-client:8080/swift/v1 | | b7df0ccdceba42ce8272d26882627e33 | RegionOne | swift | object-store | True | admin | http://ceph-rgw-client:8080/swift/v1 | | b9499be1f9ac4ad8ad9705f2e1a13847 | RegionOne | keystone | identity | True | public | http://controller:5000/v3/ | | badb42c672534b8290a37b828b758c9d | RegionOne | keystone | identity | True | admin | http://controller:5000/v3/ | +----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------+ root@controller:/home/ubuntu# 再验证 root@controller:~# . admin-openrc root@controller:~# swift list root@controller:~# . demo-openrc root@controller:~# swift list root@controller:~# 可以了。 注意：这里看到，原有的数据内容已经不存在了。但是，实际上，原admin用户是有存放数据在 ceph 中的。 这是个巨大风险点呀。 注意：同时，我们之前的原生用户 swiftuser1 不能访问了。 root@controller:/home/ubuntu# . ceph-swift-user1-openrc root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list Authorization Failure. Authorization failed: Method Not Allowed (HTTP 405) root@controller:/home/ubuntu# . admin-openrc root@controller:/home/ubuntu# swift list abcd root@controller:/home/ubuntu# 也就是说，keystone认证 开启与否，与，原生用户认证相冲突。个人感觉，应该是可以同时兼容的，但是这次mysql迁移，导致不兼容了。 下面操作，展示的就是 keystone 认证生效，前后，的对比。 root@controller:/home/ubuntu# . ceph-swift-user1-openrc root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 ceph-swiftuser1-container1-object-1.txt dao.txt guo.txt root@controller:/home/ubuntu# . admin-openrc root@controller:/home/ubuntu# swift list Authorization Failure. Authorization failed: An unexpected error prevented the server from fulfilling your request. (HTTP 500) (Request-ID: req-c8535798-84cc-4571-8624-c179f992236a) root@controller:/home/ubuntu# . admin-openrc root@controller:/home/ubuntu# swift list abcd root@controller:/home/ubuntu# . ceph-swift-user1-openrc root@controller:/home/ubuntu# swift -A http://192.168.0.134:8080/auth/v1.0 -U ${username}:${subusername} -K ${password} list swiftuser1-container1 Authorization Failure. Authorization failed: Method Not Allowed (HTTP 405) root@controller:/home/ubuntu# Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "},"docs/faq/faq-wangzhijian.html":{"url":"docs/faq/faq-wangzhijian.html","title":"常见问题-心远何方","keywords":"","body":"转自： https://blog.51cto.com/wangzhijian/2159701?source=dra 关于ceph的一些问题及解决 1.问题: # ceph health HEALTH\\_WARN application not enabled on 1 pool(s) 解决: # ceph health detail HEALTH\\_WARN application not enabled on 1 pool(s) POOL\\_APP\\_NOT\\_ENABLED application not enabled on 1 pool(s) application not enabled on pool 'kube' use 'ceph osd pool application enable ', where is 'cephfs', 'rbd', 'rgw', or freeform for custom applications. # ceph osd pool application enable kube rbd enabled application 'rbd' on pool 'kube' # ceph health HEALTH\\_OK 2.问题: # ceph -s cluster: id: e781a2e4-097d-4867-858d-bdbd3a264435 health: HEALTH\\_WARN clock skew detected on mon.ceph02, mon.ceph03 解决: ####确认NTP服务是否正常工作 # systemctl status ntpd ####修改ceph配置中的时间偏差阈值 # vim /etc/ceph/ceph.conf ###在global字段下添加: mon clock drift allowed = 2 mon clock drift warn backoff = 30 ####向需要同步的mon节点推送配置文件 # cd /etc/ceph/ # ceph-deploy --overwrite-conf config push ceph{01..03} ####重启mon服务并验证 # systemctl restart ceph-mon.target # ceph -s cluster: id: e781a2e4-097d-4867-858d-bdbd3a264435 health: HEALTH\\_OK 3.问题: # rbd map abc/zhijian --id admin rbd: sysfs write failed RBD image feature set mismatch. Try disabling features unsupported by the kernel with \"rbd feature disable\". In some cases useful info is found in syslog - try \"dmesg | tail\". rbd: map failed: (6) No such device or address 解决: 由于kernel不支持块设备镜像的一些特性，所以映射失败 # rbd feature disable abc/zhijian exclusive-lock, object-map, fast-diff, deep-flatten # rbd info abc/zhijian rbd image 'zhijian': size 1024 MB in 256 objects order 22 (4096 kB objects) block\\_name\\_prefix: rbd\\_data.1011074b0dc51 format: 2 features: layering flags: create\\_timestamp: Sun May 6 13:35:21 2018 # rbd map abc/zhijian --id admin /dev/rbd0 4.问题: # ceph osd pool delete cephfs\\_data Error EPERM: WARNING: this will \\*PERMANENTLY DESTROY\\* all data stored in pool cephfs\\_data. If you are \\*ABSOLUTELY CERTAIN\\* that is what you want, pass the pool name \\*twice\\*, followed by --yes-i-really-really-mean-it. # ceph osd pool delete cephfs\\_data cephfs\\_data --yes-i-really-really-mean-it Error EPERM: pool deletion is disabled; you must first set the mon\\_allow\\_pool\\_delete config option to true before you can destroy a pool 解决: # tail -n 2 /etc/ceph/ceph.conf \\[mon\\] mon allow pool delete = true 向需要同步的mon节点推送配置文件: # cd /etc/ceph/ # ceph-deploy --overwrite-conf config push ceph{01..03} 重启mon服务并验证: # systemctl restart ceph-mon.target # ceph osd pool delete cephfs\\_data cephfs\\_data --yes-i-really-really-mean-it pool 'cephfs\\_data' removed 5.问题: # ceph osd pool rm cephfs\\_data cephfs\\_data --yes-i-really-really-mean-it Error EBUSY: pool 'cephfs\\_data' is in use by CephFS 解决: # ceph fs ls name: cephfs, metadata pool: cephfs\\_metadata, data pools: \\[cephfs\\_data \\] # ceph fs rm cephfs --yes-i-really-mean-it Error EINVAL: all MDS daemons must be inactive before removing filesystem # systemctl stop ceph-mds.target # ceph fs rm cephfs Error EPERM: this is a DESTRUCTIVE operation and will make data in your filesystem permanently inaccessible. Add --yes-i-really-mean-it if you are sure you wish to continue. # ceph fs rm cephfs --yes-i-really-mean-it # ceph fs ls No filesystems enabled 6.问题: 使用静态PV创建pod，pod一直处于ContainerCreating状态: # kubectl get pod ceph-pod1 NAME READY STATUS RESTARTS AGE ceph-pod1 0/1 ContainerCreating 0 10s ...... # kubectl describe pod ceph-pod1 Warning FailedMount 41s (x8 over 1m) kubelet, node01 MountVolume.WaitForAttach failed for volume \"ceph-pv\" : fail to check rbd image status with: (executable file not found in $PATH), rbd output: () Warning FailedMount 0s kubelet, node01 Unable to mount volumes for pod \"ceph-pod1\\_default(14e3a07d-93a8-11e8-95f6-000c29b1ec26)\": timeout expired waiting for volumes to attach or mount for pod \"default\"/\"ceph-pod1\". list of unmounted volumes=\\[ceph-vol1\\]. list of unattached volumes=\\[ceph-vol1 default-token-v9flt\\] 解决:node节点安装最新版的ceph-common解决该问题，ceph集群使用的是最新的mimic版本，而base源的版本太陈旧，故出现该问题 7.问题: 创建动态PV，PVC一直处于pending状态: # kubectl get pvc -n ceph NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ceph-pvc Pending ceph-rbd 2m # kubectl describe pvc -n ceph ...... Warning ProvisioningFailed 27s persistentvolume-controller Failed to provision volume with StorageClass \"ceph-rbd\": failed to create rbd image: exit status 1, command output: 2018-07-31 11:10:33.395991 7faa3558b7c0 -1 did not load config file, using default settings. rbd: extraneous parameter --image-feature 解决: persistentvolume-controller 服务运行在master节点，受kube-controller-manager 控制，故master节点也需要安装ceph-common包 Ceph实践指南 | Ceph实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiuapp.github.io 2017-2019 all right reserved，powered by Gitbook Updated at 2019-04-09 09:00:25 "}}